{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1123c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(os.path.abspath(\"D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\"))\n",
    "\n",
    "from utils.model_utils import get_model, get_trainable_layers\n",
    "from utils.data_loading import get_dataloaders\n",
    "from utils.utils_transforms import get_transform  \n",
    "from utils.training_utils import fine_tune_last_n_layers, train_model, get_criterion, get_optimizer, get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = \"resnet18\"\n",
    "selected_transform = \"resnet18\"\n",
    "N_max=282\n",
    "use_patches=True\n",
    "pretrained=True\n",
    "depth=2\n",
    "num_epochs=10\n",
    "batch_size=32\n",
    "learning_rate=0.001\n",
    "input_filename=\"train_df_patches_cc.csv\"\n",
    "criterion_name=\"CrossEntropyLoss\"\n",
    "criterion = get_criterion(\"CrossEntropyLoss\")\n",
    "optimizer_name = \"Adam\"\n",
    "num_classes = 2  # Change this to match your dataset\n",
    "early_stopping=10\n",
    "scheduler_name = 'no_scheduling'#CosineAnnealingLR'\n",
    "checkpoint_path = \"D:\\\\burtm\\Visual_studio_code\\PD_related_projects\\checkpoints\\\\\"\n",
    "models_path = \"D:\\\\burtm\\Visual_studio_code\\PD_related_projects\\outputs\\models\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a097da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training metadata\n",
    "training_metadata = {\n",
    "    \"type_of_approach\": \"fine tuning imagenet pre-trained model\",\n",
    "    \"type_of_approach_sigla\": \"FTIPM\",\n",
    "    \"model_name\": selected_model,\n",
    "    \"transform_name\": selected_transform,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"optimizer\": optimizer_name,\n",
    "    \"pretrained\": pretrained,\n",
    "    \"depth\": depth,\n",
    "    \"use_patches\": use_patches,\n",
    "    \"input_filename\": input_filename,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"criterion_name\": criterion_name,\n",
    "    \"early_stopping\": early_stopping,\n",
    "    \"N_max\": N_max,\n",
    "    \"scheduler_name\": scheduler_name,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e68f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=get_transform(selected_transform,use_patches=use_patches)\n",
    "train_dataloader,val_dataloader=get_dataloaders(transform, batch_size=batch_size, N_max=N_max, file_name=input_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220ebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the final classification layer (assuming you have 10 classes)\n",
    "\n",
    "model=get_model(selected_model, pretrained=pretrained, num_classes=num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is: \",device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf10c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers but the last n\n",
    "num_trainable_layers = get_trainable_layers(selected_model,depth=depth) \n",
    "model = fine_tune_last_n_layers(model, num_trainable_layers)\n",
    "optimizer = get_optimizer(model, optimizer_name, lr=learning_rate)\n",
    "scheduler = get_scheduler(optimizer, scheduler_name, T_max=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643abf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time=datetime.now()\n",
    "model,train_losses,val_losses = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, device, \n",
    "                                            num_epochs=num_epochs, \n",
    "                                            checkpoint_path=checkpoint_path,\n",
    "                                            early_stopping_patience=early_stopping, scheduler=scheduler)\n",
    "end_time=datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c826f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the best_checkpoint.pth and add the training metadata to it\n",
    "checkpoint = torch.load(checkpoint_path+'best_checkpoint.pth')\n",
    "checkpoint['training_metadata'] = training_metadata\n",
    "val_accuracy= checkpoint['val_acc']\n",
    "# Save the updated checkpoint with metadata\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "#do the same for the last checkpoint\n",
    "checkpoint = torch.load(checkpoint_path+'last_checkpoint.pth')\n",
    "checkpoint['training_metadata'] = training_metadata\n",
    "# Save the updated checkpoint with metadata\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "torch.save(checkpoint, f\"{models_path}\\{training_metadata['type_of_approach_sigla']}_ValAcc{val_accuracy}_{timestamp}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeneralPurposeML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
