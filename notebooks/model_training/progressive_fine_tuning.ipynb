{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1123c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "sys.path.append(os.path.abspath(\"D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\"))\n",
    "\n",
    "from utils.model_utils import get_model, get_trainable_layers\n",
    "from utils.data_loading import get_dataloaders\n",
    "from utils.utils_transforms import get_transform  \n",
    "from utils.training_utils import fine_tune_last_n_layers, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = \"resnet18\"\n",
    "selected_transform = \"resnet18\"\n",
    "N_max=282\n",
    "use_patches=True\n",
    "pretrained=True\n",
    "depth=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e68f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=get_transform(selected_transform,use_patches=use_patches)\n",
    "train_dataloader,val_dataloader=get_dataloaders(transform, batch_size=16, N_max=282, file_name='train_df_patches_cc.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220ebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the final classification layer (assuming you have 10 classes)\n",
    "num_classes = 2  # Change this to match your dataset\n",
    "\n",
    "model=get_model(selected_model, pretrained=pretrained, num_classes=num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is: \",device)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad0f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressive_unfreezing(model,model_name, train_loader,val_loader, \n",
    "                           criterion, device, base_lr=1e-3, num_epochs_per_stage=2, checkpoint_path=None):\n",
    "    all_train_losses, all_val_losses = [], []\n",
    "    # Freeze all layers except the classifier\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    if model_name=='resnet18':\n",
    "        for param in model.fc.parameters(): #for resnet the last concolutional layer is called fc\n",
    "            param.requires_grad = True\n",
    "        # Define optimizer for the classifier only\n",
    "        optimizer = optim.Adam(model.fc.parameters(), lr=base_lr)\n",
    "        layer_groups = [model.layer4, model.layer3, model.layer2, model.layer1]  # Deepest to shallowest layers\n",
    "    else:\n",
    "        print('model not supported')\n",
    "        return\n",
    "    \n",
    "    print('Step 1: training classification head')\n",
    "    model, train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, \n",
    "                                                  optimizer, device, num_epochs=num_epochs_per_stage,\n",
    "                                                  checkpoint_path=checkpoint_path+'\\classifier.pth')\n",
    "    all_train_losses.extend(train_losses)\n",
    "    all_val_losses.extend(val_losses)\n",
    "    print('model on:',next(model.parameters()).device)\n",
    "\n",
    "    # **Step 2: Unfreeze layers progressively**\n",
    "    print('Step 2: Unfreeze layers progressively')\n",
    "    lr = base_lr * 0.1  # Reduce learning rate for deeper layers\n",
    "\n",
    "    for i,layer in enumerate(layer_groups):\n",
    "        # Unfreeze the current layer\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True  \n",
    "\n",
    "        # Define new optimizer with unfrozen layers\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "        # Train again with the newly unfrozen layer\n",
    "        print(f\"\\nUnfreezing {layer} and training...\")\n",
    "        model, train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, \n",
    "                                                      optimizer, device, num_epochs=num_epochs_per_stage,\n",
    "                                                      checkpoint_path=checkpoint_path+f'fine_tuning_layer_{i}.pth')\n",
    "        all_train_losses.extend(train_losses)\n",
    "        all_val_losses.extend(val_losses)\n",
    "        print('model on:',next(model.parameters()).device)\n",
    "        # Decrease learning rate for stability\n",
    "        lr *= 0.1  \n",
    "    print('fine tuning complete')\n",
    "    return model, all_train_losses, all_val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643abf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_losses,val_losses = progressive_unfreezing(model,selected_model, train_dataloader,val_dataloader, \n",
    "                                                        criterion, device, base_lr=1e-3, num_epochs_per_stage=2, \n",
    "                                                        checkpoint_path='D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\\\\checkpoints')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
