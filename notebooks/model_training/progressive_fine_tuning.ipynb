{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1123c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "sys.path.append(os.path.abspath(\"D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = \"trocr-small-stage1\"\n",
    "selected_transform = \"trocr-small-stage1\"\n",
    "N_max=282\n",
    "use_patches=True\n",
    "pretrained=True\n",
    "depth=2\n",
    "hugging=True\n",
    "num_epochs=10\n",
    "batch_size=32\n",
    "learning_rate=0.001\n",
    "input_filename=\"icdar_train_df_patches_cc.csv\"\n",
    "criterion_name=\"CrossEntropyLoss\"\n",
    "criterion = training_utils.get_criterion(criterion_name)\n",
    "optimizer_name = \"Adam\"\n",
    "num_classes = 2  # Change this to match your dataset\n",
    "early_stopping=10\n",
    "scheduler_name = 'no_scheduling'#CosineAnnealingLR'\n",
    "checkpoint_path = \"D:\\\\burtm\\Visual_studio_code\\PD_related_projects\\checkpoints\\\\\"\n",
    "models_path = \"D:\\\\burtm\\Visual_studio_code\\PD_related_projects\\outputs\\models\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e011475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training metadata\n",
    "training_metadata = {\n",
    "    \"type_of_approach\": f\"progressive fine tuning of {selected_model}\",\n",
    "    \"type_of_approach_sigla\": \"pft\",\n",
    "    \"model_name\": selected_model,\n",
    "    \"transform_name\": selected_transform,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"optimizer\": optimizer_name,\n",
    "    \"pretrained\": pretrained,\n",
    "    \"depth\": depth,\n",
    "    \"use_patches\": use_patches,\n",
    "    \"input_filename\": input_filename,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"criterion_name\": criterion_name,\n",
    "    \"early_stopping\": early_stopping,\n",
    "    \"N_max\": N_max,\n",
    "    \"scheduler_name\": scheduler_name,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e68f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=utils_transforms.get_transform(selected_transform,use_patches=use_patches)\n",
    "train_dataloader,val_dataloader=data_loading.get_dataloaders(transform, batch_size=16, N_max=282, \n",
    "                                                file_name='icdar_train_df_patches_complete_cc.csv',huggingface=hugging)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220ebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the final classification layer (assuming you have 10 classes)\n",
    "num_classes = 2  # Change this to match your dataset\n",
    "\n",
    "model=model_utils.get_model(selected_model, pretrained=pretrained, num_classes=num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is: \",device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = training_utils.get_optimizer(model, optimizer_name, lr=learning_rate)\n",
    "scheduler = training_utils.get_scheduler(optimizer, scheduler_name, T_max=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad0f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_groups(name):\n",
    "    if name=='resnet18':\n",
    "        layer_groups = [model.fc,model.layer4, model.layer3, model.layer2, model.layer1]  # Deepest to shallowest layers\n",
    "    elif name=='trocr-small-stage1':\n",
    "        layer_groups = [model.encoder.layer[11], model.encoder.layer[10], model.encoder.layer[9], \n",
    "                        model.encoder.layer[8], model.encoder.layer[7], model.encoder.layer[6], model.encoder.layer[5],\n",
    "                        model.encoder.layer[4], model.encoder.layer[3], model.encoder.layer[2], model.encoder.layer[1],\n",
    "                        model.encoder.layer[0]]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {name}\")\n",
    "    return layer_groups\n",
    "\n",
    "\n",
    "\n",
    "def progressive_unfreezing(model,model_name, train_loader,val_loader, \n",
    "                           criterion, device, base_lr=1e-3, num_epochs_per_stage=2, checkpoint_path=None, optimizer_name='Adam'):\n",
    "    all_train_losses, all_val_losses = [], []\n",
    "    # Freeze all layers except the classifier\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Get the layer groups for progressive unfreezing\n",
    "    layer_groups = get_layer_groups(model_name)\n",
    "\n",
    "    # Unfreeze the classifier\n",
    "    classifier_head=layer_groups[0]\n",
    "    for param in classifier_head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Define optimizer for the classifier only\n",
    "    optimizer = training_utils.get_optimizer(classifier_head.parameters(), optimizer_name, lr=base_lr)\n",
    "    \n",
    "    print('Step 1: training classification head')\n",
    "    model, train_losses, val_losses = training_utils.train_model(model, train_loader, val_loader, criterion, \n",
    "                                                  optimizer, device, num_epochs=num_epochs_per_stage,\n",
    "                                                  checkpoint_path=checkpoint_path+'\\classifier.pth')\n",
    "    all_train_losses.extend(train_losses)\n",
    "    all_val_losses.extend(val_losses)\n",
    "    print('model on:',next(model.parameters()).device)\n",
    "\n",
    "    # **Step 2: Unfreeze layers progressively**\n",
    "    print('Step 2: Unfreeze layers progressively')\n",
    "    lr = base_lr * 0.1  # Reduce learning rate for deeper layers\n",
    "\n",
    "    for i,layer in enumerate(layer_groups[1:]):\n",
    "        # Unfreeze the current layer\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True  \n",
    "\n",
    "        # Define new optimizer with unfrozen layers\n",
    "        optimizer = training_utils.get_optimizer(filter(lambda p: p.requires_grad, model.parameters()), optimizer_name, lr=base_lr)\n",
    "\n",
    "        # Train again with the newly unfrozen layer\n",
    "        print(f\"\\nUnfreezing {layer} and training...\")\n",
    "        model, train_losses, val_losses = training_utils.train_model(model, train_loader, val_loader, criterion, \n",
    "                                                      optimizer, device, num_epochs=num_epochs_per_stage,\n",
    "                                                      checkpoint_path=checkpoint_path+f'fine_tuning_layer_{i}.pth')\n",
    "        all_train_losses.extend(train_losses)\n",
    "        all_val_losses.extend(val_losses)\n",
    "        print('model on:',next(model.parameters()).device)\n",
    "        # Decrease learning rate for stability\n",
    "        lr *= 0.1  \n",
    "    print('fine tuning complete')\n",
    "    return model, all_train_losses, all_val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643abf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_losses,val_losses = progressive_unfreezing(model,selected_model, train_dataloader,val_dataloader, \n",
    "                                                        criterion, device, base_lr=1e-3, num_epochs_per_stage=2, \n",
    "                                                        checkpoint_path='D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\\\\checkpoints',\n",
    "                                                        optimizer_name=optimizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0230e65",
   "metadata": {},
   "source": [
    "# easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1214e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_modules():\n",
    "    import importlib\n",
    "    import utils.data_loading as data_loading\n",
    "    import utils.visualization as visualization\n",
    "    import utils.dataframes as dataframes\n",
    "    import utils.utils_transforms as u_transforms\n",
    "    import utils.training_utils as training_utils\n",
    "    import utils.model_utils as model_utils\n",
    "    \n",
    "\n",
    "    importlib.reload(data_loading)\n",
    "    importlib.reload(visualization)\n",
    "    importlib.reload(dataframes)\n",
    "    importlib.reload(u_transforms)\n",
    "    importlib.reload(model_utils)\n",
    "    importlib.reload(training_utils)\n",
    "\n",
    "    return data_loading, visualization, dataframes, u_transforms, training_utils, model_utils\n",
    "data_loading, visualization, dataframes, u_transforms, training_utils, model_utils = reload_modules()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
