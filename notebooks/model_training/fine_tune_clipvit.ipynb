{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cce28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import io\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from transformers import CLIPModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "source_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(source_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f915bbd",
   "metadata": {},
   "source": [
    "# definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "699a2803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(CustomMLP, self).__init__()\n",
    "        layers = []\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_size = hidden_size\n",
    "        layers.append(nn.Linear(input_size, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "class WrappedModel(torch.nn.Module):\n",
    "    def __init__(self, model, custom_mlp,type_of_output='cls'):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.type_of_output = type_of_output\n",
    "        self.mlp = custom_mlp\n",
    "\n",
    "    def forward(self, x):\n",
    "        image_features = self.model.get_image_features(x)\n",
    "        # Normalize the features (optional but common)\n",
    "        image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        out = self.mlp(image_features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d2f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14c9865f",
   "metadata": {},
   "source": [
    "## scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7fed3d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_schedule_with_warmup(optimizer, warmup_epochs, total_epochs):\n",
    "    def lr_lambda(current_epoch):\n",
    "        if current_epoch < warmup_epochs:\n",
    "            return float(current_epoch) / float(max(1, warmup_epochs))\n",
    "        progress = float(current_epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n",
    "        return 0.5 * (1. + math.cos(math.pi * progress))\n",
    "    return LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6becb2",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e00ed9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    device,\n",
    "    base_lr,\n",
    "    weight_decay=0.01,\n",
    "    warmup_epochs=10,\n",
    "    total_epochs=100,\n",
    "    checkpoint_path='checkpoint.pt',\n",
    "    plot_every=5,\n",
    "    early_stopping_patience=10\n",
    "):\n",
    "    model = model.to(device)\n",
    "    def get_model_size_mb(model):\n",
    "        buffer = io.BytesIO()\n",
    "        torch.save(model.state_dict(), buffer)\n",
    "        size_mb = buffer.getbuffer().nbytes / 1e6\n",
    "        return size_mb\n",
    "\n",
    "    model_size_mb = get_model_size_mb(model)\n",
    "    print(f\"Model size: {model_size_mb:.2f} MB\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, warmup_epochs=warmup_epochs, total_epochs=total_epochs\n",
    "    )\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    start_epoch = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    lrs = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # Optional Resume from checkpoint\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"ðŸ”„ Resuming from checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        train_losses = checkpoint.get('train_losses', [])\n",
    "        val_losses = checkpoint.get('val_losses', [])\n",
    "        lrs = checkpoint.get('lrs', [])\n",
    "    else:\n",
    "        print(f\"ðŸ“‚ No checkpoint found at {checkpoint_path}. Starting fresh training.\")\n",
    "\n",
    "    for epoch in range(start_epoch, total_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        correct, total = 0, 0\n",
    "        for idx,batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{total_epochs} [Train]\")):\n",
    "            x, labels = batch['image'].to(device), batch['label'].to(device)\n",
    "            outputs=model(x)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            '''if idx % 50 == 0:  # Print every 100 batches\n",
    "                before = torch.cuda.memory_allocated() / 1e6\n",
    "                before_reserved = torch.cuda.memory_reserved() / 1e6\n",
    "                print(f\"[GPU Memory] Allocated: {before:.2f} MB | Reserved: {before_reserved:.2f} MB\")'''\n",
    "\n",
    "        avg_train_loss = epoch_train_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        train_acc = correct / total\n",
    "        train_accuracies.append(train_acc)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{total_epochs} [Val]\"):\n",
    "                x, labels = batch['image'].to(device), batch['label'].to(device)\n",
    "                outputs = model(x)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                epoch_val_loss += loss.item()\n",
    "                _, preds = outputs.max(1)\n",
    "                correct += preds.eq(labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        avg_val_loss = epoch_val_loss / len(val_dataloader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_acc = correct / total\n",
    "        val_accuracies.append(val_acc)\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}, Val Loss: {avg_val_loss:.4f}, Train Acc: {val_acc:.4f}\")\n",
    "        # Logging\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {lrs[-1]:.6f}\")\n",
    "\n",
    "        # Checkpointing based on val loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'lrs': lrs,\n",
    "                'train_accuracies': train_accuracies,\n",
    "                'val_accuracies': val_accuracies,\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            epochs_without_improvement = 0\n",
    "            print(f\"âœ… Saved new best model at epoch {epoch+1}\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"â³ No improvement for {epochs_without_improvement} epoch(s)\")\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(f\"â›” Early stopping at epoch {epoch+1} (no improvement for {early_stopping_patience} epochs)\")\n",
    "            break\n",
    "\n",
    "        # Plot every `plot_every` epochs\n",
    "        if (epoch + 1) % plot_every == 0 or (epoch + 1) == total_epochs:\n",
    "            plt.figure(figsize=(15, 5))\n",
    "\n",
    "            # Loss plot\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.plot(train_losses, label=\"Train Loss\")\n",
    "            plt.plot(val_losses, label=\"Val Loss\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend()\n",
    "\n",
    "            # Accuracy plot\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.plot(train_accuracies, label=\"Train Acc\")\n",
    "            plt.plot(val_accuracies, label=\"Val Acc\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "            plt.legend()\n",
    "\n",
    "            # Learning rate plot\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.plot(lrs, label=\"Learning Rate\", color='orange')\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"LR\")\n",
    "            plt.legend()\n",
    "\n",
    "            plt.suptitle(f\"Epoch {epoch+1}\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f21a27cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                device, num_epochs=5, checkpoint_path=None,early_stopping_patience=10, scheduler=None\n",
    "                ,data_type='image'):\n",
    "    start_time=datetime.now()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        # Training Loop\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = batch[data_type], batch['label']\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "        # Validation Loop\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, labels = batch[data_type], batch['label']\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, preds = outputs.max(1)\n",
    "                correct += preds.eq(labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Save checkpoint if loss improves\n",
    "        if checkpoint_path and avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            checkpoint = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': best_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'train_loss': avg_train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'epoch': epoch,\n",
    "                'time_from_start': datetime.now()-start_time,\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path+'best_checkpoint.pth')\n",
    "            print(f\"Checkpoint saved: {checkpoint_path}\"+'best_checkpoint.pth')\n",
    "        else:\n",
    "            checkpoint = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': best_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'train_loss': avg_train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'epoch': epoch,\n",
    "                'time_from_start': datetime.now()-start_time,\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path+'last_checkpoint.pth')\n",
    "            print(f\"Checkpoint saved: {checkpoint_path}\"+'last_checkpoint.pth')\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "    \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf053fac",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cbdf04f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPatchDataset(Dataset):\n",
    "    def __init__(self, df, label_column,transform=None,huggingface=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dirs (list of str): List of directories to load images from.\n",
    "            labels_df (DataFrame): DataFrame containing labeled images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.image_files = df['file_name'].tolist()\n",
    "        self.img_labels = df[label_column].tolist()\n",
    "        self.img_writers = df['writer'].tolist()\n",
    "        self.x1 = df['x'].tolist()\n",
    "        self.y1 = df['y'].tolist()\n",
    "        self.x2 = df['x2'].tolist()\n",
    "        self.y2 = df['y2'].tolist()\n",
    "        self.transform = transform\n",
    "        self.huggingface = huggingface\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        x1=self.x1[idx]\n",
    "        y1=self.y1[idx]\n",
    "        x2=self.x2[idx]\n",
    "        y2=self.y2[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        patch = image.crop((x1, y1, x2, y2))\n",
    "        writer=self.img_writers[idx]\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        if self.huggingface:\n",
    "            # the transform is actually an huggingface processor in this case\n",
    "            inputs = self.transform(images=patch, return_tensors=\"pt\")\n",
    "            # Remove batch dimension from inputs\n",
    "            patch = inputs['pixel_values'].squeeze()\n",
    "        else:\n",
    "            if self.transform:\n",
    "                patch = self.transform(patch)\n",
    "\n",
    "        return {\n",
    "            'image': patch,\n",
    "            'writer': int(writer),\n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81869dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "745856e9",
   "metadata": {},
   "source": [
    "# initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b2164409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1281f446",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches=True\n",
    "input_filename='icdar_train_df_patches_20250515_164130.csv' #gw=5 m_patches=5\n",
    "huggingface=True\n",
    "pooling=False # if true in transformer mdoels use pooling, if false only the cls token\n",
    "custom_transform=False\n",
    "transform_mode='resize'\n",
    "selected_model = 'clip-vit-large-patch14'\n",
    "truncation = 'remove head'\n",
    "running = 'new-laptop'\n",
    "saved = 'old-laptop'\n",
    "num_classes = 2\n",
    "hidden_size = 1024\n",
    "out_clip = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d1d9846",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.01\n",
    "base_lr = 3e-5\n",
    "warmup_epochs = 10\n",
    "total_epochs = 100\n",
    "patience = 10\n",
    "batch_size=8\n",
    "checkpoint_path = f\"{source_path}\\\\outputs\\\\models\\\\clip-vit\\\\checkpoint.pt\"\n",
    "p_train = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4fbe5943",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = u_transforms.get_transform(selected_model, use_patches=patches, custom=custom_transform, mode=transform_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb935374",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(f'openai/clip-vit-large-patch14') # probably i need to train with reduced precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9845cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_mlp = CustomMLP(input_size=out_clip, hidden_sizes=[hidden_size], output_size=num_classes)\n",
    "model = WrappedModel(model, custom_mlp, type_of_output='cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "048d22d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{source_path}\\\\outputs\\\\preprocessed_data\\\\{input_filename}\")\n",
    "train_df=file_IO.change_filename_from_to(train_df, fr=saved, to=running)\n",
    "train_df = train_df.drop(columns=['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e3550158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>writer</th>\n",
       "      <th>isEng</th>\n",
       "      <th>same_text</th>\n",
       "      <th>file_name</th>\n",
       "      <th>male</th>\n",
       "      <th>index</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>n_cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>986</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>986</td>\n",
       "      <td>986</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1479</td>\n",
       "      <td>493</td>\n",
       "      <td>1972</td>\n",
       "      <td>986</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>986</td>\n",
       "      <td>493</td>\n",
       "      <td>1479</td>\n",
       "      <td>986</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1972</td>\n",
       "      <td>493</td>\n",
       "      <td>2465</td>\n",
       "      <td>986</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   writer  isEng  same_text  \\\n",
       "0       1      0          0   \n",
       "1       1      0          0   \n",
       "2       1      0          0   \n",
       "3       1      0          0   \n",
       "4       1      0          0   \n",
       "\n",
       "                                           file_name  male  index     x    y  \\\n",
       "0  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      0     0  493   \n",
       "1  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      1   493  493   \n",
       "2  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      2  1479  493   \n",
       "3  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      3   986  493   \n",
       "4  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      4  1972  493   \n",
       "\n",
       "     x2   y2  n_cc  \n",
       "0   493  986   111  \n",
       "1   986  986    96  \n",
       "2  1972  986    99  \n",
       "3  1479  986    90  \n",
       "4  2465  986    99  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de2b8445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>writer</th>\n",
       "      <th>isEng</th>\n",
       "      <th>same_text</th>\n",
       "      <th>file_name</th>\n",
       "      <th>male</th>\n",
       "      <th>index</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>n_cc</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>986</td>\n",
       "      <td>111</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>986</td>\n",
       "      <td>986</td>\n",
       "      <td>96</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1479</td>\n",
       "      <td>493</td>\n",
       "      <td>1972</td>\n",
       "      <td>986</td>\n",
       "      <td>99</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>986</td>\n",
       "      <td>493</td>\n",
       "      <td>1479</td>\n",
       "      <td>986</td>\n",
       "      <td>90</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1972</td>\n",
       "      <td>493</td>\n",
       "      <td>2465</td>\n",
       "      <td>986</td>\n",
       "      <td>99</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   writer  isEng  same_text  \\\n",
       "0       1      0          0   \n",
       "1       1      0          0   \n",
       "2       1      0          0   \n",
       "3       1      0          0   \n",
       "4       1      0          0   \n",
       "\n",
       "                                           file_name  male  index     x    y  \\\n",
       "0  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      0     0  493   \n",
       "1  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      1   493  493   \n",
       "2  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      2  1479  493   \n",
       "3  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      3   986  493   \n",
       "4  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      4  1972  493   \n",
       "\n",
       "     x2   y2  n_cc  train  \n",
       "0   493  986   111    1.0  \n",
       "1   986  986    96    1.0  \n",
       "2  1972  986    99    1.0  \n",
       "3  1479  986    90    1.0  \n",
       "4  2465  986    99    1.0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the probability of being 0\n",
    "N = 5 #train_df['writer'].nunique()\n",
    "\n",
    "# Create a dataframe with writer column from 1 to 282\n",
    "pages_df = pd.DataFrame({'writer': np.arange(1, N+1)})\n",
    "\n",
    "# Add a train column that is randomly 0 or 1 with probability p of being 0\n",
    "pages_df['train'] = np.random.choice([0, 1], size=len(pages_df), p=[1-p_train, p_train])\n",
    "\n",
    "# Merge with the train_df dataframe on the writer column\n",
    "train_df = train_df.merge(pages_df, on='writer', how='left')\n",
    "\n",
    "# Display the dataframe\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6b36cfe1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m CustomPatchDataset(train_df[(train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwriter\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39mN_max)] , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmale\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform, huggingface\u001b[38;5;241m=\u001b[39mhuggingface)\n\u001b[1;32m----> 6\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\GeneralPurposeML\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:385\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 385\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\GeneralPurposeML\\lib\\site-packages\\torch\\utils\\data\\sampler.py:156\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m     )\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "N_max=N\n",
    "train_dataset = CustomPatchDataset(train_df[(train_df['train']==1) & (train_df['writer']<=N_max)], 'male' ,transform=transform, huggingface=huggingface)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomPatchDataset(train_df[(train_df['train']==0) & (train_df['writer']<=N_max)] , 'male', transform=transform, huggingface=huggingface)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa8b6bc",
   "metadata": {},
   "source": [
    "# run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cf3c8e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "[GPU Memory] Allocated: 17.04 MB | Reserved: 41.94 MB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "print(f\"[GPU Memory] Allocated: {torch.cuda.memory_allocated() / 1e6:.2f} MB | Reserved: {torch.cuda.memory_reserved() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ade8226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 428,406,019\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_params = count_parameters(model)\n",
    "print(f\"Number of trainable parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f90d80a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 1713.86 MB\n",
      "ðŸ“‚ No checkpoint found at c:\\Users\\andre\\VsCode\\PD related projects\\gender_detection\\outputs\\models\\clip-vit\\checkpoint.pt. Starting fresh training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [03:03<00:00, 14.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6904, Train Acc: 0.7900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val Loss: 0.6933, Train Acc: 0.5000\n",
      "Epoch 1 | Train Loss: 0.6904 | Val Loss: 0.6933 | LR: 0.000000\n",
      "âœ… Saved new best model at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 [Train]:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [02:16<01:00, 15.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_lr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[54], line 65\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, val_dataloader, device, base_lr, weight_decay, warmup_epochs, total_epochs, checkpoint_path, plot_every, early_stopping_patience)\u001b[0m\n\u001b[0;32m     62\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m     64\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 65\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     68\u001b[0m epoch_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\GeneralPurposeML\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\GeneralPurposeML\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\GeneralPurposeML\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    device,\n",
    "    base_lr,\n",
    "    weight_decay=weight_decay,\n",
    "    warmup_epochs=warmup_epochs,\n",
    "    total_epochs=total_epochs,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    plot_every=5,\n",
    "    early_stopping_patience=patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953334a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60c4e8",
   "metadata": {},
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40bff4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(f'openai/clip-vit-large-patch14') # probably i need to train with reduced precision\n",
    "custom_mlp = CustomMLP(input_size=768, hidden_sizes=[hidden_size], output_size=num_classes)\n",
    "input = torch.randn(2, 3, 224, 224)  # Use actual image tensors or pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b083dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "print(model.get_image_features(input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "081d0c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.vision_model.embeddings.class_embedding has gradients\n",
      "model.vision_model.embeddings.patch_embedding.weight has gradients\n",
      "model.vision_model.embeddings.position_embedding.weight has gradients\n",
      "model.vision_model.pre_layrnorm.weight has gradients\n",
      "model.vision_model.pre_layrnorm.bias has gradients\n",
      "model.vision_model.encoder.layers.0.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.0.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.0.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.0.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.0.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.0.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.0.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.0.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.0.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.0.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.0.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.0.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.0.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.0.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.0.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.0.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.1.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.1.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.1.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.1.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.1.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.1.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.1.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.1.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.1.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.1.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.1.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.1.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.1.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.1.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.1.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.1.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.2.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.2.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.2.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.2.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.2.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.2.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.2.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.2.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.2.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.2.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.2.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.2.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.2.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.2.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.2.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.2.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.3.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.3.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.3.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.3.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.3.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.3.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.3.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.3.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.3.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.3.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.3.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.3.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.3.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.3.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.3.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.3.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.4.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.4.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.4.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.4.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.4.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.4.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.4.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.4.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.4.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.4.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.4.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.4.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.4.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.4.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.4.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.4.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.5.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.5.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.5.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.5.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.5.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.5.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.5.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.5.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.5.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.5.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.5.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.5.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.5.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.5.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.5.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.5.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.6.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.6.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.6.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.6.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.6.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.6.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.6.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.6.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.6.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.6.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.6.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.6.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.6.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.6.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.6.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.6.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.7.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.7.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.7.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.7.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.7.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.7.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.7.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.7.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.7.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.7.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.7.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.7.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.7.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.7.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.7.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.7.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.8.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.8.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.8.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.8.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.8.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.8.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.8.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.8.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.8.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.8.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.8.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.8.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.8.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.8.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.8.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.8.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.9.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.9.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.9.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.9.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.9.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.9.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.9.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.9.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.9.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.9.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.9.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.9.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.9.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.9.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.9.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.9.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.10.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.10.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.10.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.10.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.10.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.10.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.10.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.10.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.10.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.10.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.10.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.10.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.10.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.10.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.10.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.10.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.11.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.11.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.11.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.11.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.11.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.11.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.11.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.11.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.11.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.11.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.11.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.11.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.11.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.11.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.11.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.11.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.12.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.12.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.12.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.12.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.12.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.12.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.12.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.12.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.12.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.12.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.12.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.12.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.12.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.12.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.12.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.12.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.13.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.13.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.13.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.13.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.13.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.13.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.13.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.13.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.13.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.13.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.13.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.13.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.13.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.13.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.13.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.13.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.14.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.14.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.14.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.14.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.14.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.14.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.14.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.14.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.14.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.14.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.14.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.14.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.14.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.14.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.14.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.14.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.15.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.15.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.15.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.15.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.15.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.15.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.15.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.15.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.15.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.15.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.15.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.15.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.15.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.15.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.15.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.15.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.16.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.16.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.16.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.16.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.16.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.16.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.16.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.16.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.16.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.16.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.16.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.16.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.16.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.16.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.16.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.16.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.17.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.17.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.17.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.17.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.17.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.17.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.17.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.17.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.17.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.17.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.17.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.17.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.17.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.17.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.17.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.17.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.18.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.18.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.18.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.18.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.18.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.18.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.18.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.18.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.18.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.18.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.18.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.18.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.18.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.18.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.18.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.18.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.19.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.19.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.19.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.19.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.19.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.19.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.19.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.19.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.19.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.19.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.19.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.19.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.19.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.19.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.19.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.19.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.20.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.20.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.20.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.20.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.20.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.20.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.20.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.20.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.20.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.20.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.20.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.20.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.20.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.20.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.20.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.20.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.21.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.21.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.21.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.21.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.21.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.21.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.21.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.21.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.21.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.21.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.21.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.21.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.21.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.21.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.21.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.21.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.22.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.22.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.22.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.22.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.22.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.22.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.22.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.22.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.22.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.22.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.22.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.22.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.22.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.22.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.22.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.22.layer_norm2.bias has gradients\n",
      "model.vision_model.encoder.layers.23.self_attn.k_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.23.self_attn.k_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.23.self_attn.v_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.23.self_attn.v_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.23.self_attn.q_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.23.self_attn.q_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.23.self_attn.out_proj.weight has gradients\n",
      "model.vision_model.encoder.layers.23.self_attn.out_proj.bias has gradients\n",
      "model.vision_model.encoder.layers.23.layer_norm1.weight has gradients\n",
      "model.vision_model.encoder.layers.23.layer_norm1.bias has gradients\n",
      "model.vision_model.encoder.layers.23.mlp.fc1.weight has gradients\n",
      "model.vision_model.encoder.layers.23.mlp.fc1.bias has gradients\n",
      "model.vision_model.encoder.layers.23.mlp.fc2.weight has gradients\n",
      "model.vision_model.encoder.layers.23.mlp.fc2.bias has gradients\n",
      "model.vision_model.encoder.layers.23.layer_norm2.weight has gradients\n",
      "model.vision_model.encoder.layers.23.layer_norm2.bias has gradients\n",
      "model.vision_model.post_layernorm.weight has gradients\n",
      "model.vision_model.post_layernorm.bias has gradients\n",
      "model.visual_projection.weight has gradients\n",
      "mlp.model.0.weight has gradients\n",
      "mlp.model.0.bias has gradients\n",
      "mlp.model.2.weight has gradients\n",
      "mlp.model.2.bias has gradients\n"
     ]
    }
   ],
   "source": [
    "#flow check\n",
    "model = WrappedModel(model, custom_mlp, type_of_output='cls')\n",
    "\n",
    "output = model(input)\n",
    "loss = torch.nn.CrossEntropyLoss()(output, torch.tensor([0, 1]))  # Example labels\n",
    "loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(name, \"has gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d9bb85",
   "metadata": {},
   "source": [
    "# easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a20615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_modules():\n",
    "    import importlib\n",
    "    import utils.image_processing as image_processing\n",
    "    import utils.file_IO as file_IO\n",
    "    import utils.visualization as visualization\n",
    "    import utils.tests as tests\n",
    "    import utils.data_loading as data_loading\n",
    "    import utils.utils_transforms as u_transforms\n",
    "\n",
    "    importlib.reload(data_loading)\n",
    "    importlib.reload(file_IO)\n",
    "    importlib.reload(image_processing)\n",
    "    importlib.reload(visualization)\n",
    "    importlib.reload(tests)\n",
    "    importlib.reload(u_transforms)\n",
    "\n",
    "    return image_processing, file_IO, visualization, tests, data_loading, u_transforms\n",
    "image_processing, file_IO, visualization, tests, data_loading, u_transforms = reload_modules()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeneralPurposeML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
