{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1123c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(os.path.abspath(\"D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5af7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = \"trocr-2transformer-blocks\"\n",
    "selected_transform = \"trocr-small-stage1\"\n",
    "N_max=282\n",
    "use_patches=True\n",
    "pretrained=True\n",
    "depth=-1\n",
    "hugging=True\n",
    "num_epochs=100\n",
    "batch_size=32\n",
    "learning_rate=0.001\n",
    "input_filename='icdar_train_df_cc_5patches_perName.csv' #'representations.h5'\n",
    "criterion_name=\"CrossEntropyLoss\"\n",
    "criterion = training_utils.get_criterion(criterion_name)\n",
    "optimizer_name = \"Adam\"\n",
    "num_classes = 2  # Change this to match your dataset\n",
    "early_stopping=10\n",
    "scheduler_name = 'no_scheduling'#CosineAnnealingLR'\n",
    "checkpoint_path = \"D:\\\\burtm\\Visual_studio_code\\PD_related_projects\\checkpoints\\\\\"\n",
    "models_path = \"D:\\\\burtm\\Visual_studio_code\\PD_related_projects\\outputs\\models\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a097da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training metadata\n",
    "training_metadata = {\n",
    "    \"type_of_approach\": \"training transformer on top of trocr extracted features\",\n",
    "    \"type_of_approach_sigla\": \"accelerated_fine_tuning_trocr\",\n",
    "    \"model_name\": selected_model,\n",
    "    \"transform_name\": selected_transform,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"optimizer\": optimizer_name,\n",
    "    \"pretrained\": pretrained,\n",
    "    \"depth\": depth,\n",
    "    \"use_patches\": use_patches,\n",
    "    \"input_filename\": input_filename,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"criterion_name\": criterion_name,\n",
    "    \"early_stopping\": early_stopping,\n",
    "    \"N_max\": N_max,\n",
    "    \"scheduler_name\": scheduler_name,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cb1e68f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded train_df from: D:\\burtm\\Visual_studio_code\\PD_related_projects\\outputs\\preprocessed_data\\icdar_train_df_cc_5patches_perName.csv\n"
     ]
    }
   ],
   "source": [
    "transform=u_transforms.get_transform(selected_transform,use_patches=use_patches)\n",
    "train_dataloader,val_dataloader=data_loading.get_dataloaders(transform, batch_size=16, N_max=282, \n",
    "                                                file_name=input_filename,hugging=hugging,h5=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53895690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([578, 384])\n"
     ]
    }
   ],
   "source": [
    "# Get one batch from the dataloader\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "# Print the shape of the 'image' element in the batch\n",
    "print(batch['features'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0220ebc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.deit.modeling_deit.DeiTModel'> is overwritten by shared encoder config: DeiTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 384,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"deit\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.48.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 384,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 1024,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.48.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 64044\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is:  cuda\n"
     ]
    }
   ],
   "source": [
    "model=model_utils.get_model(selected_model, pretrained=pretrained, num_classes=num_classes,\n",
    "                            input_size=384, hidden_sizes=[128])\n",
    "\n",
    "# Define loss function and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is: \",device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8f6ecb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullClassifier(\n",
      "  (encoder): TruncatedDeiT(\n",
      "    (transformer): ModuleList(\n",
      "      (0-1): 2 x DeiTLayer(\n",
      "        (attention): DeiTSdpaAttention(\n",
      "          (attention): DeiTSdpaSelfAttention(\n",
      "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (output): DeiTSelfOutput(\n",
      "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): DeiTIntermediate(\n",
      "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): DeiTOutput(\n",
      "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "        (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "    (pooler): DeiTPooler(\n",
      "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classification_head): CustomMLP(\n",
      "    (model): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d0693587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Define the computation performed at every call.\n",
      "\n",
      "Should be overridden by all subclasses.\n",
      "\n",
      ".. note::\n",
      "    Although the recipe for forward pass needs to be defined within\n",
      "    this function, one should call the :class:`Module` instance afterwards\n",
      "    instead of this since the former takes care of running the\n",
      "    registered hooks while the latter silently ignores them.\n",
      "\u001b[1;31mSource:\u001b[0m   \n",
      "    \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m#cls_token_output = outputs.last_hidden_state[:, 0, :]  # Extract the CLS token\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cls'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'mean'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_head\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFile:\u001b[0m      d:\\burtm\\visual_studio_code\\pd_related_projects\\utils\\model_utils.py\n",
      "\u001b[1;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "model.forward??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c2cf10c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Layers: 40\n",
      "Total Parameters: 3,747,074\n",
      "Trainable Parameters after freezing: 0\n",
      "\n",
      "\n",
      "Unfreezing the following layers:\n",
      "Linear(in_features=384, out_features=384, bias=True)\n",
      "Linear(in_features=384, out_features=384, bias=True)\n",
      "Linear(in_features=384, out_features=384, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=384, out_features=384, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=384, out_features=1536, bias=True)\n",
      "GELUActivation()\n",
      "Linear(in_features=1536, out_features=384, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "Linear(in_features=384, out_features=384, bias=True)\n",
      "Linear(in_features=384, out_features=384, bias=True)\n",
      "Linear(in_features=384, out_features=384, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=384, out_features=384, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=384, out_features=1536, bias=True)\n",
      "GELUActivation()\n",
      "Linear(in_features=1536, out_features=384, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "Linear(in_features=384, out_features=384, bias=True)\n",
      "Tanh()\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=128, out_features=2, bias=True)\n",
      "\n",
      "\n",
      "Trainable Parameters after UN-freezing last n layers: 3,747,074\n"
     ]
    }
   ],
   "source": [
    "# Freeze all layers but the last n\n",
    "num_trainable_layers = model_utils.get_trainable_layers(selected_model,depth=depth) \n",
    "model = training_utils.fine_tune_last_n_layers(model, num_trainable_layers)\n",
    "optimizer = training_utils.get_optimizer(model, optimizer_name, lr=learning_rate)\n",
    "scheduler = training_utils.get_scheduler(optimizer, scheduler_name, T_max=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dafcc644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 384])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m features \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#print(features[0].shape)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Apply the model to the batch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Print the outputs\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#print(outputs)\u001b[39;00m\n",
      "File \u001b[1;32md:\\burtm\\Visual_studio_code\\conda_environments\\GeneralPurposeML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\burtm\\Visual_studio_code\\conda_environments\\GeneralPurposeML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\burtm\\Visual_studio_code\\PD_related_projects\\utils\\model_utils.py:78\u001b[0m, in \u001b[0;36mFullClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 78\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     80\u001b[0m     output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# Move the batch features to the same device as the model\n",
    "features = batch['features'].to(device)\n",
    "#print(features[0].shape)\n",
    "\n",
    "# Apply the model to the batch\n",
    "outputs = model(features)\n",
    "\n",
    "# Print the outputs\n",
    "#print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643abf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 317/317 [10:12<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6988, Train Acc: 0.5091\n",
      "Epoch 1, Val Loss: 0.7025, Val Acc: 0.4483\n",
      "Checkpoint saved: D:\\burtm\\Visual_studio_code\\PD_related_projects\\checkpoints\\best_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 317/317 [09:20<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.6928, Train Acc: 0.5202\n",
      "Epoch 2, Val Loss: 0.6939, Val Acc: 0.4655\n",
      "Checkpoint saved: D:\\burtm\\Visual_studio_code\\PD_related_projects\\checkpoints\\best_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 317/317 [08:18<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.6839, Train Acc: 0.5702\n",
      "Epoch 3, Val Loss: 0.7550, Val Acc: 0.4483\n",
      "Checkpoint saved: D:\\burtm\\Visual_studio_code\\PD_related_projects\\checkpoints\\last_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 317/317 [07:46<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.6932, Train Acc: 0.5138\n",
      "Epoch 4, Val Loss: 0.6950, Val Acc: 0.4483\n",
      "Checkpoint saved: D:\\burtm\\Visual_studio_code\\PD_related_projects\\checkpoints\\last_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 317/317 [10:14<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.6929, Train Acc: 0.5138\n",
      "Epoch 5, Val Loss: 0.6955, Val Acc: 0.4483\n",
      "Checkpoint saved: D:\\burtm\\Visual_studio_code\\PD_related_projects\\checkpoints\\last_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 317/317 [08:25<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.6929, Train Acc: 0.5138\n",
      "Epoch 6, Val Loss: 0.6959, Val Acc: 0.4483\n",
      "Checkpoint saved: D:\\burtm\\Visual_studio_code\\PD_related_projects\\checkpoints\\last_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100:   5%|▌         | 16/317 [00:33<08:26,  1.68s/it]"
     ]
    }
   ],
   "source": [
    "start_time=datetime.now()\n",
    "model,train_losses,val_losses = training_utils.train_model(model, train_dataloader, val_dataloader, criterion, optimizer, device, \n",
    "                                            num_epochs=num_epochs, \n",
    "                                            checkpoint_path=checkpoint_path,\n",
    "                                            early_stopping_patience=early_stopping, scheduler=scheduler, data_type='features')\n",
    "end_time=datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c826f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the best_checkpoint.pth and add the training metadata to it\n",
    "checkpoint = torch.load(checkpoint_path+'best_checkpoint.pth')\n",
    "checkpoint['training_metadata'] = training_metadata\n",
    "val_accuracy= checkpoint['val_acc']\n",
    "# Save the updated checkpoint with metadata\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "#do the same for the last checkpoint\n",
    "checkpoint = torch.load(checkpoint_path+'last_checkpoint.pth')\n",
    "checkpoint['training_metadata'] = training_metadata\n",
    "# Save the updated checkpoint with metadata\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "torch.save(checkpoint, f\"{models_path}\\{training_metadata['type_of_approach_sigla']}_ValAcc{val_accuracy}_{timestamp}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce76e4d",
   "metadata": {},
   "source": [
    "# easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "674645d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_modules():\n",
    "    import importlib\n",
    "    import utils.data_loading as data_loading\n",
    "    import utils.visualization as visualization\n",
    "    import utils.dataframes as dataframes\n",
    "    import utils.utils_transforms as u_transforms\n",
    "    import utils.training_utils as training_utils\n",
    "    import utils.model_utils as model_utils\n",
    "    \n",
    "\n",
    "    importlib.reload(data_loading)\n",
    "    importlib.reload(visualization)\n",
    "    importlib.reload(dataframes)\n",
    "    importlib.reload(u_transforms)\n",
    "    importlib.reload(model_utils)\n",
    "    importlib.reload(training_utils)\n",
    "\n",
    "    return data_loading, visualization, dataframes, u_transforms, training_utils, model_utils\n",
    "data_loading, visualization, dataframes, u_transforms, training_utils, model_utils = reload_modules()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeneralPurposeML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
