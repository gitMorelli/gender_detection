{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#from torchinfo import summary\n",
    "#import torchvision.transforms.functional as F\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision.models import resnet50, ResNet50_Weights, resnet18, ResNet18_Weights\n",
    "#from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "source_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(source_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_resnet = transforms.Compose([\n",
    "                    transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Data Augmentation for Contrastive Learning\n",
    "# ------------------------------\n",
    "color_jitter = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)\n",
    "\n",
    "ContrastiveTransform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(256, scale=(0.8, 1.0), interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomApply([color_jitter], p=0.8),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406], \n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "# SimCLR data augmentation transform\n",
    "simclr_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
    "    ], p=0.8),\n",
    "    #transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([color_jitter], p=0.8),\n",
    "    transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0)),  # kernel_size ~ 0.1 * image size\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPatchDataset(Dataset):\n",
    "    def __init__(self, df,transform=ContrastiveTransform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dirs (list of str): List of directories to load images from.\n",
    "            labels_df (DataFrame): DataFrame containing labeled images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.image_files = df['file_name'].tolist()\n",
    "        self.x1 = df['x'].tolist()\n",
    "        self.y1 = df['y'].tolist()\n",
    "        self.x2 = df['x2'].tolist()\n",
    "        self.y2 = df['y2'].tolist()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        x1=self.x1[idx]\n",
    "        y1=self.y1[idx]\n",
    "        x2=self.x2[idx]\n",
    "        y2=self.y2[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        patch = image.crop((x1, y1, x2, y2))\n",
    "\n",
    "        if self.transform:\n",
    "            patch1 = self.transform(patch)\n",
    "            patch2 = self.transform(patch)\n",
    "\n",
    "        return {\n",
    "            'image1': patch1,\n",
    "            'image2': patch2,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveModel(nn.Module):\n",
    "    \"\"\"ResNet Backbone + Projection Head for SimCLR.\"\"\"\n",
    "    def __init__(self, model, in_features,projection_dim=128,hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.encoder = model\n",
    "        self.encoder.fc = nn.Identity()  # Remove the classification head\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        projections = self.projection_head(features)\n",
    "        return F.normalize(projections, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# NT-Xent Contrastive Loss\n",
    "# ------------------------------\n",
    "class NTXentLoss(nn.Module):\n",
    "    \"\"\"Normalized Temperature-scaled Cross Entropy Loss (SimCLR).\"\"\"\n",
    "    def __init__(self, temperature=0.5, verbose=False):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        verbose = self.verbose\n",
    "        batch_size = z_i.shape[0]\n",
    "        z = torch.cat([z_i, z_j], dim=0)  # Stack positive pairs\n",
    "        similarity_matrix = torch.matmul(z, z.T)  # Cosine similarity\n",
    "        #I don't normalize because the model already does it in the forward pass\n",
    "        \n",
    "        # Remove self-similarity\n",
    "        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)\n",
    "        similarity_matrix = similarity_matrix[~mask].view(2 * batch_size, -1)\n",
    "        if verbose:\n",
    "            print(\"similarity_matrix: \",similarity_matrix.shape)\n",
    "            print(similarity_matrix)\n",
    "        \n",
    "        # Compute positive pairs similarity\n",
    "        '''\n",
    "        positives = torch.cat([torch.diag(similarity_matrix, batch_size-1), \n",
    "                               torch.diag(similarity_matrix, -batch_size+1)], dim=0)\n",
    "        '''\n",
    "        \n",
    "        # Compute NT-Xent loss\n",
    "        #labels = torch.arange(2 * batch_size, device=z.device)\n",
    "        labels = torch.cat([torch.arange(batch_size-1,2*batch_size-1, device=z.device),\n",
    "                            torch.arange(batch_size, device=z.device)], dim=0)\n",
    "        if verbose:\n",
    "            print(\"labels: \",labels.shape)\n",
    "            print(labels)\n",
    "        \n",
    "        # Each row should have the highest score at its label index to be used by the crossentropy loss\n",
    "        loss = self.criterion(similarity_matrix / self.temperature, labels)\n",
    "        #labels should be the class indexes. The first argument are the logits.\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTXentLoss_chat(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        batch_size = z_i.size(0)\n",
    "        z = torch.cat([z_i, z_j], dim=0)  # [2N, D]\n",
    "\n",
    "        # Cosine similarity matrix\n",
    "        sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)  # [2N, 2N]\n",
    "        sim = sim / self.temperature\n",
    "\n",
    "        # Mask out self-similarities\n",
    "        mask = torch.eye(2 * batch_size, device=z.device).bool()\n",
    "        sim.masked_fill_(mask, -float('inf'))\n",
    "\n",
    "        # Positive indices: for i in [0, 2N), positive pair is at i + N (mod 2N)\n",
    "        pos_idx = torch.arange(2 * batch_size, device=z.device)\n",
    "        pos_pair_idx = (pos_idx + batch_size) % (2 * batch_size)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(sim, pos_pair_idx)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizers and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"class LARS(Optimizer):\\n    def __init__(self, params, lr, weight_decay=1e-6, momentum=0.9, eta=0.001, eps=1e-9):\\n        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum,\\n                        eta=eta, eps=eps)\\n        super(LARS, self).__init__(params, defaults)\\n\\n    def step(self):\\n        for group in self.param_groups:\\n            for p in group['params']:\\n                if p.grad is None:\\n                    continue\\n\\n                dp = p.grad.data\\n\\n                if group['weight_decay'] != 0:\\n                    dp = dp.add(group['weight_decay'], p.data)\\n\\n                param_norm = torch.norm(p.data)\\n                grad_norm = torch.norm(dp)\\n                one = torch.ones_like(param_norm)\\n\\n                q = torch.where(param_norm > 0,\\n                                torch.where(grad_norm > 0,\\n                                            (group['eta'] * param_norm / (grad_norm + group['eps'])),\\n                                            one),\\n                                one)\\n\\n                dp = dp.mul(q)\\n\\n                if group.get('momentum', 0) > 0:\\n                    param_state = self.state[p]\\n                    if 'momentum_buffer' not in param_state:\\n                        buf = param_state['momentum_buffer'] = torch.clone(dp).detach()\\n                    else:\\n                        buf = param_state['momentum_buffer']\\n                        buf.mul_(group['momentum']).add_(dp)\\n                    dp = buf\\n\\n                p.data.add_(-group['lr'], dp)\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class LARS(Optimizer):\n",
    "    def __init__(self, params, lr, weight_decay=1e-6, momentum=0.9, eta=0.001, eps=1e-9):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum,\n",
    "                        eta=eta, eps=eps)\n",
    "        super(LARS, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                dp = p.grad.data\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    dp = dp.add(group['weight_decay'], p.data)\n",
    "\n",
    "                param_norm = torch.norm(p.data)\n",
    "                grad_norm = torch.norm(dp)\n",
    "                one = torch.ones_like(param_norm)\n",
    "\n",
    "                q = torch.where(param_norm > 0,\n",
    "                                torch.where(grad_norm > 0,\n",
    "                                            (group['eta'] * param_norm / (grad_norm + group['eps'])),\n",
    "                                            one),\n",
    "                                one)\n",
    "\n",
    "                dp = dp.mul(q)\n",
    "\n",
    "                if group.get('momentum', 0) > 0:\n",
    "                    param_state = self.state[p]\n",
    "                    if 'momentum_buffer' not in param_state:\n",
    "                        buf = param_state['momentum_buffer'] = torch.clone(dp).detach()\n",
    "                    else:\n",
    "                        buf = param_state['momentum_buffer']\n",
    "                        buf.mul_(group['momentum']).add_(dp)\n",
    "                    dp = buf\n",
    "\n",
    "                p.data.add_(-group['lr'], dp)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_schedule_with_warmup(optimizer, warmup_epochs, total_epochs):\n",
    "    def lr_lambda(current_epoch):\n",
    "        if current_epoch < warmup_epochs:\n",
    "            return float(current_epoch) / float(max(1, warmup_epochs))\n",
    "        progress = float(current_epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n",
    "        return 0.5 * (1. + math.cos(math.pi * progress))\n",
    "    return LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lars(model, dataloader, device, n_epochs):\n",
    "    model = model.to(device)\n",
    "\n",
    "    base_lr = 4.8\n",
    "    batch_size = 4096\n",
    "    effective_lr = base_lr * (batch_size / 256)\n",
    "    optimizer = LARS(model.parameters(), lr=effective_lr, weight_decay=1e-6)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_epochs=10, total_epochs=100)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for (x1, x2) in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            x = torch.cat([x1, x2], dim=0).to(device)\n",
    "\n",
    "            z = model(x)\n",
    "            loss = NTXentLoss(z)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    device,\n",
    "    base_lr,\n",
    "    warmup_epochs=10,\n",
    "    total_epochs=100,\n",
    "    checkpoint_path='checkpoint.pt',\n",
    "    plot_every=5,\n",
    "    early_stopping_patience=10, loss_fn=NTXentLoss(temperature=0.5),\n",
    "):\n",
    "    model = model.to(device)\n",
    "    def get_model_size_mb(model):\n",
    "        buffer = io.BytesIO()\n",
    "        torch.save(model.state_dict(), buffer)\n",
    "        size_mb = buffer.getbuffer().nbytes / 1e6\n",
    "        return size_mb\n",
    "\n",
    "    model_size_mb = get_model_size_mb(model)\n",
    "    print(f\"Model size: {model_size_mb:.2f} MB\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=base_lr)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, warmup_epochs=warmup_epochs, total_epochs=total_epochs\n",
    "    )\n",
    "    loss_fn = loss_fn\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    start_epoch = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    lrs = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # Optional Resume from checkpoint\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"ðŸ”„ Resuming from checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        train_losses = checkpoint.get('train_losses', [])\n",
    "        val_losses = checkpoint.get('val_losses', [])\n",
    "        lrs = checkpoint.get('lrs', [])\n",
    "    else:\n",
    "        print(f\"ðŸ“‚ No checkpoint found at {checkpoint_path}. Starting fresh training.\")\n",
    "\n",
    "    for epoch in range(start_epoch, total_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "\n",
    "        for idx,batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{total_epochs} [Train]\")):\n",
    "            x_i, x_j = batch['image1'].to(device), batch['image2'].to(device)\n",
    "            z_i, z_j = model(x_i), model(x_j)\n",
    "            loss = loss_fn(z_i, z_j)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            '''del x_i, x_j, z_i, z_j, loss  # clear references\n",
    "            torch.cuda.empty_cache()  # optional, helps if memory gets fragmented'''\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "            '''if idx % 50 == 0:  # Print every 100 batches\n",
    "                before = torch.cuda.memory_allocated() / 1e6\n",
    "                before_reserved = torch.cuda.memory_reserved() / 1e6\n",
    "                print(f\"[GPU Memory] Allocated: {before:.2f} MB | Reserved: {before_reserved:.2f} MB\")'''\n",
    "\n",
    "        avg_train_loss = epoch_train_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{total_epochs} [Val]\"):\n",
    "                x_i, x_j = batch['image1'].to(device), batch['image2'].to(device)\n",
    "                z_i, z_j = model(x_i), model(x_j)\n",
    "                loss = loss_fn(z_i, z_j)\n",
    "                epoch_val_loss += loss.item()\n",
    "        avg_val_loss = epoch_val_loss / len(val_dataloader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Logging\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {lrs[-1]:.6f}\")\n",
    "\n",
    "        # Checkpointing based on val loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'lrs': lrs\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            epochs_without_improvement = 0\n",
    "            print(f\"âœ… Saved new best model at epoch {epoch+1}\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"â³ No improvement for {epochs_without_improvement} epoch(s)\")\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(f\"â›” Early stopping at epoch {epoch+1} (no improvement for {early_stopping_patience} epochs)\")\n",
    "            break\n",
    "\n",
    "        # Plot every `plot_every` epochs\n",
    "        if (epoch + 1) % plot_every == 0 or (epoch + 1) == total_epochs:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(train_losses, label=\"Train Loss\")\n",
    "            plt.plot(val_losses, label=\"Val Loss\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(lrs, label=\"Learning Rate\", color='orange')\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"LR\")\n",
    "            plt.legend()\n",
    "\n",
    "            plt.suptitle(f\"Epoch {epoch+1}\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_contrastive(model, dataloader, optimizer, device, epochs=10):\n",
    "    model.train()\n",
    "    loss_fn = NTXentLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            x_i, x_j = batch['image1'], batch['image2']\n",
    "            x_i, x_j = x_i.to(device), x_j.to(device)\n",
    "            z_i, z_j = model(x_i), model(x_j)\n",
    "            loss = loss_fn(z_i, z_j)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-sne visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# After pretraining\\nfeatures, labels = [], []\\nwith torch.no_grad():\\n    for x, y in dataloader:\\n        x = x.to(device)\\n        z = model(x)\\n        features.append(z.cpu())\\n        labels.append(y)\\n# Plot t-SNE of concatenated features'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# After pretraining\n",
    "features, labels = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device)\n",
    "        z = model(x)\n",
    "        features.append(z.cpu())\n",
    "        labels.append(y)\n",
    "# Plot t-SNE of concatenated features'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport gc\\nimport torch\\nimport objgraph\\n\\ngc.collect()\\ntensors = [obj for obj in gc.get_objects() if torch.is_tensor(obj) and obj.is_cuda]\\n\\nprint(f\"Found {len(tensors)} CUDA tensors occupying memory:\")\\nfor i, t in enumerate(tensors):\\nprint(f\"Tensor {i}: shape={t.shape}, dtype={t.dtype}, size={t.element_size() * t.nelement() / 1024**2:.2f} MB\")'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_size_of_image_on_model(model, device, input_shape=(3, 224, 224), contrastive=True,batches=5):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    before = torch.cuda.memory_allocated() / 1e6\n",
    "    before_reserved = torch.cuda.memory_reserved() / 1e6\n",
    "    print(f\"[GPU Memory] Allocated before model loading: {before:.2f} MB | Reserved: {before_reserved:.2f} MB\")\n",
    "    model = model.to(device)\n",
    "    after = torch.cuda.memory_allocated() / 1e6\n",
    "    after_reserved = torch.cuda.memory_reserved() / 1e6\n",
    "    print(f\"[GPU Memory] Allocated after model loading: {after:.2f} MB | Reserved: {after_reserved:.2f} MB\")\n",
    "    print(f\"[GPU Memory] DELTA: {after-before:.2f} MB | Reserved: {after_reserved-before_reserved:.2f} MB\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, warmup_epochs=10, total_epochs=100\n",
    "    )\n",
    "    loss_fn = NTXentLoss()\n",
    "\n",
    "    model.train()\n",
    "    batch = {\n",
    "        'image1': torch.rand((batches, *input_shape), device=device),\n",
    "        'image2': torch.rand((batches, *input_shape), device=device)\n",
    "    }\n",
    "    x_i, x_j = batch['image1'].to(device), batch['image2'].to(device)\n",
    "    allocated_image = tensor_memory_mb(x_i)\n",
    "    print(f\"x_i memory: {allocated_image:.2f} MB\")\n",
    "    print('batch size:', x_i.shape[0])\n",
    "\n",
    "    before = torch.cuda.memory_allocated() / 1e6\n",
    "    before_reserved = torch.cuda.memory_reserved() / 1e6\n",
    "    print(f\"[GPU Memory] Allocated before model forward pass: {before:.2f} MB | Reserved: {before_reserved:.2f} MB\")\n",
    "    z_i, z_j = model(x_i), model(x_j)\n",
    "    after = torch.cuda.memory_allocated() / 1e6\n",
    "    after_reserved = torch.cuda.memory_reserved() / 1e6\n",
    "    print(f\"[GPU Memory] Allocated after model forward pass: {after:.2f} MB | Reserved: {after_reserved:.2f} MB\")\n",
    "    print(f\"[GPU Memory] DELTA: {after-before:.2f} MB | Reserved: {after_reserved-before_reserved:.2f} MB\")\n",
    "    print(\"-\" * 50)\n",
    "    if contrastive:\n",
    "        loss = loss_fn(z_i, z_j)\n",
    "    else:\n",
    "        loss = F.mse_loss(z_i, z_j)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    before = torch.cuda.memory_allocated() / 1e6\n",
    "    before_reserved = torch.cuda.memory_reserved() / 1e6\n",
    "    print(f\"[GPU Memory] Allocated before backward pass: {before:.2f} MB | Reserved: {before_reserved:.2f} MB\")\n",
    "    loss.backward()\n",
    "    after = torch.cuda.memory_allocated() / 1e6\n",
    "    after_reserved = torch.cuda.memory_reserved() / 1e6\n",
    "    print(f\"[GPU Memory] Allocated after backward pass: {after:.2f} MB | Reserved: {after_reserved:.2f} MB\")\n",
    "    print(f\"[GPU Memory] DELTA: {after-before:.2f} MB | Reserved: {after_reserved-before_reserved:.2f} MB\")\n",
    "    print(\"-\" * 50)\n",
    "'''\n",
    "import gc\n",
    "import torch\n",
    "import objgraph\n",
    "\n",
    "gc.collect()\n",
    "tensors = [obj for obj in gc.get_objects() if torch.is_tensor(obj) and obj.is_cuda]\n",
    "\n",
    "print(f\"Found {len(tensors)} CUDA tensors occupying memory:\")\n",
    "for i, t in enumerate(tensors):\n",
    "print(f\"Tensor {i}: shape={t.shape}, dtype={t.dtype}, size={t.element_size() * t.nelement() / 1024**2:.2f} MB\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU Memory] Allocated before model loading: 551.73 MB | Reserved: 893.39 MB\n",
      "[GPU Memory] Allocated after model loading: 649.74 MB | Reserved: 905.97 MB\n",
      "[GPU Memory] DELTA: 98.02 MB | Reserved: 12.58 MB\n",
      "--------------------------------------------------\n",
      "x_i memory: 0.57 MB\n",
      "batch size: 1\n",
      "[GPU Memory] Allocated before model forward pass: 650.95 MB | Reserved: 908.07 MB\n",
      "[GPU Memory] Allocated after model forward pass: 823.77 MB | Reserved: 985.66 MB\n",
      "[GPU Memory] DELTA: 172.82 MB | Reserved: 77.59 MB\n",
      "--------------------------------------------------\n",
      "[GPU Memory] Allocated before backward pass: 823.77 MB | Reserved: 985.66 MB\n",
      "[GPU Memory] Allocated after backward pass: 747.74 MB | Reserved: 985.66 MB\n",
      "[GPU Memory] DELTA: -76.03 MB | Reserved: 0.00 MB\n",
      "--------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "[GPU Memory] Allocated before model loading: 551.73 MB | Reserved: 893.39 MB\n",
      "[GPU Memory] Allocated after model loading: 649.74 MB | Reserved: 905.97 MB\n",
      "[GPU Memory] DELTA: 98.02 MB | Reserved: 12.58 MB\n",
      "--------------------------------------------------\n",
      "x_i memory: 18.38 MB\n",
      "batch size: 32\n",
      "[GPU Memory] Allocated before model forward pass: 688.28 MB | Reserved: 905.97 MB\n",
      "[GPU Memory] Allocated after model forward pass: 6179.58 MB | Reserved: 6341.79 MB\n",
      "[GPU Memory] DELTA: 5491.30 MB | Reserved: 5435.82 MB\n",
      "--------------------------------------------------\n",
      "[GPU Memory] Allocated before backward pass: 6179.64 MB | Reserved: 6341.79 MB\n",
      "[GPU Memory] Allocated after backward pass: 787.72 MB | Reserved: 6964.64 MB\n",
      "[GPU Memory] DELTA: -5391.91 MB | Reserved: 622.85 MB\n",
      "--------------------------------------------------\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 538796 KiB |   5924 MiB | 190808 MiB | 190282 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 538796 KiB |   5924 MiB | 190808 MiB | 190282 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 524749 KiB |   5880 MiB | 188473 MiB | 187960 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   6642 MiB |   6642 MiB |  46002 MiB |  39360 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 333651 KiB | 405229 KiB |  92754 MiB |  92428 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1303    |    2071    |   54160    |   52857    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1303    |    2071    |   54160    |   52857    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     237    |     237    |    2270    |    2033    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      54    |     117    |   21981    |   21927    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_size_of_image_on_model(\n",
    "    ContrastiveModel(resnet50(weights=ResNet50_Weights.IMAGENET1K_V2), in_features=2048, projection_dim=128),\n",
    "    device='cuda',\n",
    "    input_shape=(3, 224, 224), contrastive = True, batches = 1 \n",
    ")\n",
    "print(\"$$\" * 50)\n",
    "get_size_of_image_on_model(\n",
    "    ContrastiveModel(resnet50(weights=ResNet50_Weights.IMAGENET1K_V2), in_features=2048, projection_dim=128),\n",
    "    device='cuda',\n",
    "    input_shape=(3, 224, 224), contrastive = True, batches = 32\n",
    ")\n",
    "print(torch.cuda.memory_summary(device='cuda', abbreviated=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU Memory] Allocated before model loading: 551.73 MB | Reserved: 893.39 MB\n",
      "[GPU Memory] Allocated after model loading: 597.47 MB | Reserved: 893.39 MB\n",
      "[GPU Memory] DELTA: 45.74 MB | Reserved: 0.00 MB\n",
      "--------------------------------------------------\n",
      "x_i memory: 2.87 MB\n",
      "batch size: 5\n",
      "[GPU Memory] Allocated before model forward pass: 603.89 MB | Reserved: 893.39 MB\n",
      "[GPU Memory] Allocated after model forward pass: 822.09 MB | Reserved: 920.65 MB\n",
      "[GPU Memory] DELTA: 218.20 MB | Reserved: 27.26 MB\n",
      "--------------------------------------------------\n",
      "[GPU Memory] Allocated before backward pass: 822.10 MB | Reserved: 920.65 MB\n",
      "[GPU Memory] Allocated after backward pass: 649.84 MB | Reserved: 922.75 MB\n",
      "[GPU Memory] DELTA: -172.25 MB | Reserved: 2.10 MB\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "get_size_of_image_on_model(\n",
    "    ContrastiveModel(resnet18(weights='IMAGENET1K_V1'), in_features=512, projection_dim=128),\n",
    "    device='cuda',\n",
    "    input_shape=(3, 224, 224)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file=\"icdar_train_df_iam_rimes_patches_20250615_170212.csv\"\n",
    "running = 'new-laptop'\n",
    "saved = 'new-laptop'\n",
    "pretrained = True\n",
    "selected_model='resnet50'\n",
    "train_df = pd.read_csv(f\"{source_path}\\\\outputs\\\\preprocessed_data\\\\{source_file}\")\n",
    "train_df=file_IO.change_filename_from_to(train_df, fr=saved, to=running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cahtgpt suggests\n",
    "initial_lr = 3e-4\n",
    "weight_decay=1e-6\n",
    "batch_size = 32\n",
    "temperature=0.5\n",
    "checkpoint_path = f\"{source_path}\\\\outputs\\\\models\\\\contrastive\\\\checkpoint.pt\"\n",
    "p_train=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model in_features: 2048\n"
     ]
    }
   ],
   "source": [
    "if selected_model == 'resnet50':\n",
    "    weights = ResNet50_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "    model = resnet50(weights=weights)\n",
    "else:\n",
    "    raise ValueError(f\"Model {selected_model} is not supported.\")\n",
    "in_features = model.fc.in_features\n",
    "print(f\"Model in_features: {in_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>source</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>n_cc</th>\n",
       "      <th>black_ratio</th>\n",
       "      <th>index</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\rimes\\images_blocs...</td>\n",
       "      <td>rimes</td>\n",
       "      <td>381</td>\n",
       "      <td>1524</td>\n",
       "      <td>762</td>\n",
       "      <td>1905</td>\n",
       "      <td>13</td>\n",
       "      <td>0.332700</td>\n",
       "      <td>1573</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\rimes\\images_blocs...</td>\n",
       "      <td>rimes</td>\n",
       "      <td>762</td>\n",
       "      <td>1143</td>\n",
       "      <td>1143</td>\n",
       "      <td>1524</td>\n",
       "      <td>18</td>\n",
       "      <td>0.293157</td>\n",
       "      <td>1573</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\rimes\\images_blocs...</td>\n",
       "      <td>rimes</td>\n",
       "      <td>762</td>\n",
       "      <td>1524</td>\n",
       "      <td>1143</td>\n",
       "      <td>1905</td>\n",
       "      <td>16</td>\n",
       "      <td>0.276273</td>\n",
       "      <td>1573</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\iam offline\\forms\\...</td>\n",
       "      <td>iam</td>\n",
       "      <td>1239</td>\n",
       "      <td>1298</td>\n",
       "      <td>1652</td>\n",
       "      <td>1711</td>\n",
       "      <td>18</td>\n",
       "      <td>0.265980</td>\n",
       "      <td>37</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\rimes\\images_blocs...</td>\n",
       "      <td>rimes</td>\n",
       "      <td>762</td>\n",
       "      <td>381</td>\n",
       "      <td>1143</td>\n",
       "      <td>762</td>\n",
       "      <td>18</td>\n",
       "      <td>0.265002</td>\n",
       "      <td>1573</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name source     x     y    x2  \\\n",
       "0  C:\\Users\\andre\\PhD\\Datasets\\rimes\\images_blocs...  rimes   381  1524   762   \n",
       "1  C:\\Users\\andre\\PhD\\Datasets\\rimes\\images_blocs...  rimes   762  1143  1143   \n",
       "2  C:\\Users\\andre\\PhD\\Datasets\\rimes\\images_blocs...  rimes   762  1524  1143   \n",
       "3  C:\\Users\\andre\\PhD\\Datasets\\iam offline\\forms\\...    iam  1239  1298  1652   \n",
       "4  C:\\Users\\andre\\PhD\\Datasets\\rimes\\images_blocs...  rimes   762   381  1143   \n",
       "\n",
       "     y2  n_cc  black_ratio  index  train  \n",
       "0  1905    13     0.332700   1573    1.0  \n",
       "1  1524    18     0.293157   1573    1.0  \n",
       "2  1905    16     0.276273   1573    1.0  \n",
       "3  1711    18     0.265980     37    1.0  \n",
       "4   762    18     0.265002   1573    1.0  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the probability of being 0\n",
    "N = train_df['index'].nunique()\n",
    "\n",
    "# Create a dataframe with writer column from 1 to 282\n",
    "pages_df = pd.DataFrame({'index': np.arange(1, N+1)})\n",
    "\n",
    "# Add a train column that is randomly 0 or 1 with probability p of being 0\n",
    "pages_df['train'] = np.random.choice([0, 1], size=len(pages_df), p=[1-p_train, p_train])\n",
    "\n",
    "# Merge with the train_df dataframe on the writer column\n",
    "train_df = train_df.merge(pages_df, on='index', how='left')\n",
    "\n",
    "# Display the dataframe\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_max=N\n",
    "train_dataset = CustomPatchDataset(train_df[(train_df['train']==1) & (train_df['index']<=N_max)] ,transform=simclr_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomPatchDataset(train_df[(train_df['train']==0) & (train_df['index']<=N_max)] , transform=simclr_transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Create an iterator\\ndata_iter = iter(train_dataloader)\\n# Get a single batch\\nbatch = next(data_iter)\\nplot_image_batches(batch['image1'], batch['image2'])\""
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Create an iterator\n",
    "data_iter = iter(train_dataloader)\n",
    "# Get a single batch\n",
    "batch = next(data_iter)\n",
    "plot_image_batches(batch['image1'], batch['image2'])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_model = ContrastiveModel(model, in_features=in_features,projection_dim=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "[GPU Memory] Allocated: 1518.65 MB | Reserved: 7446.99 MB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(f\"[GPU Memory] Allocated: {torch.cuda.memory_allocated() / 1e6:.2f} MB | Reserved: {torch.cuda.memory_reserved() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 96.58 MB\n",
      "ðŸ“‚ No checkpoint found at c:\\Users\\andre\\VsCode\\PD related projects\\gender_detection\\outputs\\models\\contrastive\\checkpoint.pt. Starting fresh training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1002/1002 [28:52<00:00,  1.73s/it]\n",
      "Epoch 1/100 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [02:40<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 4.0986 | Val Loss: 4.0966 | LR: 0.000000\n",
      "âœ… Saved new best model at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1002/1002 [28:40<00:00,  1.72s/it]\n",
      "Epoch 2/100 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [02:47<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 3.0675 | Val Loss: 2.9551 | LR: 0.000030\n",
      "âœ… Saved new best model at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1002/1002 [34:41<00:00,  2.08s/it]\n",
      "Epoch 3/100 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [02:48<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 2.9110 | Val Loss: 2.9085 | LR: 0.000060\n",
      "âœ… Saved new best model at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1002/1002 [28:49<00:00,  1.73s/it]\n",
      "Epoch 4/100 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [02:44<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 2.8845 | Val Loss: 2.9175 | LR: 0.000090\n",
      "â³ No improvement for 1 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 [Train]:   3%|â–Ž         | 29/1002 [00:50<28:27,  1.75s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[157], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrastive_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_lr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNTXentLoss_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[148], line 56\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, val_dataloader, device, base_lr, warmup_epochs, total_epochs, checkpoint_path, plot_every, early_stopping_patience, loss_fn)\u001b[0m\n\u001b[0;32m     53\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     54\u001b[0m epoch_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx,batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [Train]\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m     57\u001b[0m     x_i, x_j \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     58\u001b[0m     z_i, z_j \u001b[38;5;241m=\u001b[39m model(x_i), model(x_j)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\GeneralPurposeML\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\GeneralPurposeML\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\GeneralPurposeML\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\GeneralPurposeML\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\GeneralPurposeML\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\GeneralPurposeML\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m--> 171\u001b[0m         {\n\u001b[0;32m    172\u001b[0m             key: collate(\n\u001b[0;32m    173\u001b[0m                 [d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map\n\u001b[0;32m    174\u001b[0m             )\n\u001b[0;32m    175\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[0;32m    176\u001b[0m         }\n\u001b[0;32m    177\u001b[0m     )\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\GeneralPurposeML\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    171\u001b[0m         {\n\u001b[1;32m--> 172\u001b[0m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[0;32m    176\u001b[0m         }\n\u001b[0;32m    177\u001b[0m     )\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\GeneralPurposeML\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\GeneralPurposeML\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    contrastive_model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    device,\n",
    "    initial_lr,\n",
    "    warmup_epochs=10,\n",
    "    total_epochs=100,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    plot_every=5,\n",
    "    early_stopping_patience=15, loss_fn=NTXentLoss_chat(temperature=0.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mipc_collect()\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\GeneralPurposeML\\lib\\site-packages\\torch\\cuda\\memory.py:222\u001b[0m, in \u001b[0;36mempty_cache\u001b[1;34m()\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 222\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 16 17:45:53 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.52                 Driver Version: 576.52         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   40C    P8              7W /   55W |    7935MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            9352      C   ...s\\GeneralPurposeML\\python.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_dir = os.path.join(source_path, \"outputs\", \"logs\")\n",
    "# Example usage:\n",
    "LOG_OUT_FILE = out_dir+\"\\\\feature_extraction_metadata_log.json\"\n",
    "print(f\"Log file path: {LOG_OUT_FILE}\")\n",
    "file_IO.add_or_update_experiment(\n",
    "    experiment, LOG_OUT_FILE,\n",
    "    custom_metadata={\n",
    "        \"original raw file\": source_data,\n",
    "        \"input file\": input_file_name,\n",
    "        \"FE model\": model_used,\n",
    "        \"FE transform\": transform_used,\n",
    "        \"classifier model\": selected_model,\n",
    "        \"model_params\": feature_extraction_model[selected_model].get_params(),\n",
    "        \"n_splits\": n_splits,\n",
    "        \"train_on_language\": train_on_language,\n",
    "        \"train_on_same\": train_on_same,\n",
    "        \"task\": task,\n",
    "        \"with cross validation\": with_cross_validation,\n",
    "        \"with PCA\": with_pca,\n",
    "        \"training time for cross-validation\": time_taken_cross_val,\n",
    "        \"training time for final model\": time_taken,\n",
    "        \"cross_val_accuracies\": cross_val_accuracies,\n",
    "        \"subgroup_accuracies\": subgroup_accuracies,\n",
    "        \"is_kaggle\": is_kaggle,\n",
    "        \"test\": 'this is a test column',\n",
    "        \"description\": ''' I am training a classifier on the feature vectors extracted by a deep model\n",
    "        I am evaluating the results on subsets of the training data, based on language and same/different text.''' \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch tensor1 shape: torch.Size([15, 3, 256, 256])\n",
      "Batch tensor2 shape: torch.Size([15, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "class RandomTensorDataset(Dataset):\n",
    "    def __init__(self, num_samples, image_size=(256, 256, 3)):\n",
    "        self.num_samples = num_samples\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor1 = torch.rand(*self.image_size).permute(2, 0, 1)  # Convert to (C, H, W)\n",
    "        #tensor2 = torch.rand(*self.image_size).permute(2, 0, 1)  # Convert to (C, H, W)\n",
    "        tensor2 = tensor1.clone()\n",
    "        return tensor1, tensor2\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "random_dataset = RandomTensorDataset(num_samples=1000)\n",
    "random_dataloader = DataLoader(random_dataset, batch_size=15, shuffle=True)\n",
    "\n",
    "# Example: Fetch a batch\n",
    "random_batch = next(iter(random_dataloader))\n",
    "print(f\"Batch tensor1 shape: {random_batch[0].shape}\")\n",
    "print(f\"Batch tensor2 shape: {random_batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ResNet50_Weights.IMAGENET1K_V1 \n",
    "model = resnet50(weights=weights)\n",
    "contrastive_model = ContrastiveModel(model, in_features=2048, projection_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch representation1 shape: torch.Size([15, 128])\n",
      "Batch representation2 shape: torch.Size([15, 128])\n"
     ]
    }
   ],
   "source": [
    "x_i,x_j = random_batch[0],random_batch[1]\n",
    "z_i, z_j = contrastive_model(x_i), contrastive_model(x_j)\n",
    "print(f\"Batch representation1 shape: {z_i.shape}\")\n",
    "print(f\"Batch representation2 shape: {z_j.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(z_i[0]-z_j[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity_matrix:  torch.Size([30, 29])\n",
      "tensor([[0.8167, 0.8420, 0.8287, 0.8028, 0.8629, 0.7963, 0.8248, 0.7954, 0.8657,\n",
      "         0.8111, 0.8016, 0.8066, 0.8522, 0.8174, 1.0000, 0.8167, 0.8420, 0.8287,\n",
      "         0.8028, 0.8629, 0.7963, 0.8248, 0.7954, 0.8657, 0.8111, 0.8016, 0.8066,\n",
      "         0.8522, 0.8174],\n",
      "        [0.8167, 0.8960, 0.8702, 0.8418, 0.8371, 0.8495, 0.8287, 0.7845, 0.8557,\n",
      "         0.8522, 0.8757, 0.8707, 0.8549, 0.8304, 0.8167, 1.0000, 0.8960, 0.8702,\n",
      "         0.8418, 0.8371, 0.8495, 0.8287, 0.7845, 0.8557, 0.8522, 0.8757, 0.8707,\n",
      "         0.8549, 0.8304],\n",
      "        [0.8420, 0.8960, 0.9105, 0.9053, 0.8980, 0.8769, 0.8923, 0.8544, 0.9001,\n",
      "         0.9072, 0.9003, 0.8951, 0.8972, 0.8924, 0.8420, 0.8960, 1.0000, 0.9105,\n",
      "         0.9053, 0.8980, 0.8769, 0.8923, 0.8544, 0.9001, 0.9072, 0.9003, 0.8951,\n",
      "         0.8972, 0.8924],\n",
      "        [0.8287, 0.8702, 0.9105, 0.8745, 0.8567, 0.8216, 0.8514, 0.8208, 0.8700,\n",
      "         0.8743, 0.8419, 0.8471, 0.8542, 0.8427, 0.8287, 0.8702, 0.9105, 1.0000,\n",
      "         0.8745, 0.8567, 0.8216, 0.8514, 0.8208, 0.8700, 0.8743, 0.8419, 0.8471,\n",
      "         0.8542, 0.8427],\n",
      "        [0.8028, 0.8418, 0.9053, 0.8745, 0.8482, 0.8621, 0.8733, 0.8560, 0.8877,\n",
      "         0.8860, 0.8420, 0.8758, 0.8639, 0.8966, 0.8028, 0.8418, 0.9053, 0.8745,\n",
      "         1.0000, 0.8482, 0.8621, 0.8733, 0.8560, 0.8877, 0.8860, 0.8420, 0.8758,\n",
      "         0.8639, 0.8966],\n",
      "        [0.8629, 0.8371, 0.8980, 0.8567, 0.8482, 0.8232, 0.8735, 0.8488, 0.8874,\n",
      "         0.8623, 0.8479, 0.8773, 0.8830, 0.8757, 0.8629, 0.8371, 0.8980, 0.8567,\n",
      "         0.8482, 1.0000, 0.8232, 0.8735, 0.8488, 0.8874, 0.8623, 0.8479, 0.8773,\n",
      "         0.8830, 0.8757],\n",
      "        [0.7963, 0.8495, 0.8769, 0.8216, 0.8621, 0.8232, 0.8639, 0.8353, 0.8728,\n",
      "         0.8345, 0.8894, 0.8468, 0.8350, 0.8668, 0.7963, 0.8495, 0.8769, 0.8216,\n",
      "         0.8621, 0.8232, 1.0000, 0.8639, 0.8353, 0.8728, 0.8345, 0.8894, 0.8468,\n",
      "         0.8350, 0.8668],\n",
      "        [0.8248, 0.8287, 0.8923, 0.8514, 0.8733, 0.8735, 0.8639, 0.8923, 0.9073,\n",
      "         0.8325, 0.8360, 0.8141, 0.8703, 0.8655, 0.8248, 0.8287, 0.8923, 0.8514,\n",
      "         0.8733, 0.8735, 0.8639, 1.0000, 0.8923, 0.9073, 0.8325, 0.8360, 0.8141,\n",
      "         0.8703, 0.8655],\n",
      "        [0.7954, 0.7845, 0.8544, 0.8208, 0.8560, 0.8488, 0.8353, 0.8923, 0.8526,\n",
      "         0.7961, 0.8152, 0.8145, 0.8503, 0.8266, 0.7954, 0.7845, 0.8544, 0.8208,\n",
      "         0.8560, 0.8488, 0.8353, 0.8923, 1.0000, 0.8526, 0.7961, 0.8152, 0.8145,\n",
      "         0.8503, 0.8266],\n",
      "        [0.8657, 0.8557, 0.9001, 0.8700, 0.8877, 0.8874, 0.8728, 0.9073, 0.8526,\n",
      "         0.8161, 0.8547, 0.8297, 0.8906, 0.8599, 0.8657, 0.8557, 0.9001, 0.8700,\n",
      "         0.8877, 0.8874, 0.8728, 0.9073, 0.8526, 1.0000, 0.8161, 0.8547, 0.8297,\n",
      "         0.8906, 0.8599],\n",
      "        [0.8111, 0.8522, 0.9072, 0.8743, 0.8860, 0.8623, 0.8345, 0.8325, 0.7961,\n",
      "         0.8161, 0.8109, 0.8897, 0.8540, 0.8843, 0.8111, 0.8522, 0.9072, 0.8743,\n",
      "         0.8860, 0.8623, 0.8345, 0.8325, 0.7961, 0.8161, 1.0000, 0.8109, 0.8897,\n",
      "         0.8540, 0.8843],\n",
      "        [0.8016, 0.8757, 0.9003, 0.8419, 0.8420, 0.8479, 0.8894, 0.8360, 0.8152,\n",
      "         0.8547, 0.8109, 0.8603, 0.8776, 0.8545, 0.8016, 0.8757, 0.9003, 0.8419,\n",
      "         0.8420, 0.8479, 0.8894, 0.8360, 0.8152, 0.8547, 0.8109, 1.0000, 0.8603,\n",
      "         0.8776, 0.8545],\n",
      "        [0.8066, 0.8707, 0.8951, 0.8471, 0.8758, 0.8773, 0.8468, 0.8141, 0.8145,\n",
      "         0.8297, 0.8897, 0.8603, 0.8703, 0.8922, 0.8066, 0.8707, 0.8951, 0.8471,\n",
      "         0.8758, 0.8773, 0.8468, 0.8141, 0.8145, 0.8297, 0.8897, 0.8603, 1.0000,\n",
      "         0.8703, 0.8922],\n",
      "        [0.8522, 0.8549, 0.8972, 0.8542, 0.8639, 0.8830, 0.8350, 0.8703, 0.8503,\n",
      "         0.8906, 0.8540, 0.8776, 0.8703, 0.8779, 0.8522, 0.8549, 0.8972, 0.8542,\n",
      "         0.8639, 0.8830, 0.8350, 0.8703, 0.8503, 0.8906, 0.8540, 0.8776, 0.8703,\n",
      "         1.0000, 0.8779],\n",
      "        [0.8174, 0.8304, 0.8924, 0.8427, 0.8966, 0.8757, 0.8668, 0.8655, 0.8266,\n",
      "         0.8599, 0.8843, 0.8545, 0.8922, 0.8779, 0.8174, 0.8304, 0.8924, 0.8427,\n",
      "         0.8966, 0.8757, 0.8668, 0.8655, 0.8266, 0.8599, 0.8843, 0.8545, 0.8922,\n",
      "         0.8779, 1.0000],\n",
      "        [1.0000, 0.8167, 0.8420, 0.8287, 0.8028, 0.8629, 0.7963, 0.8248, 0.7954,\n",
      "         0.8657, 0.8111, 0.8016, 0.8066, 0.8522, 0.8174, 0.8167, 0.8420, 0.8287,\n",
      "         0.8028, 0.8629, 0.7963, 0.8248, 0.7954, 0.8657, 0.8111, 0.8016, 0.8066,\n",
      "         0.8522, 0.8174],\n",
      "        [0.8167, 1.0000, 0.8960, 0.8702, 0.8418, 0.8371, 0.8495, 0.8287, 0.7845,\n",
      "         0.8557, 0.8522, 0.8757, 0.8707, 0.8549, 0.8304, 0.8167, 0.8960, 0.8702,\n",
      "         0.8418, 0.8371, 0.8495, 0.8287, 0.7845, 0.8557, 0.8522, 0.8757, 0.8707,\n",
      "         0.8549, 0.8304],\n",
      "        [0.8420, 0.8960, 1.0000, 0.9105, 0.9053, 0.8980, 0.8769, 0.8923, 0.8544,\n",
      "         0.9001, 0.9072, 0.9003, 0.8951, 0.8972, 0.8924, 0.8420, 0.8960, 0.9105,\n",
      "         0.9053, 0.8980, 0.8769, 0.8923, 0.8544, 0.9001, 0.9072, 0.9003, 0.8951,\n",
      "         0.8972, 0.8924],\n",
      "        [0.8287, 0.8702, 0.9105, 1.0000, 0.8745, 0.8567, 0.8216, 0.8514, 0.8208,\n",
      "         0.8700, 0.8743, 0.8419, 0.8471, 0.8542, 0.8427, 0.8287, 0.8702, 0.9105,\n",
      "         0.8745, 0.8567, 0.8216, 0.8514, 0.8208, 0.8700, 0.8743, 0.8419, 0.8471,\n",
      "         0.8542, 0.8427],\n",
      "        [0.8028, 0.8418, 0.9053, 0.8745, 1.0000, 0.8482, 0.8621, 0.8733, 0.8560,\n",
      "         0.8877, 0.8860, 0.8420, 0.8758, 0.8639, 0.8966, 0.8028, 0.8418, 0.9053,\n",
      "         0.8745, 0.8482, 0.8621, 0.8733, 0.8560, 0.8877, 0.8860, 0.8420, 0.8758,\n",
      "         0.8639, 0.8966],\n",
      "        [0.8629, 0.8371, 0.8980, 0.8567, 0.8482, 1.0000, 0.8232, 0.8735, 0.8488,\n",
      "         0.8874, 0.8623, 0.8479, 0.8773, 0.8830, 0.8757, 0.8629, 0.8371, 0.8980,\n",
      "         0.8567, 0.8482, 0.8232, 0.8735, 0.8488, 0.8874, 0.8623, 0.8479, 0.8773,\n",
      "         0.8830, 0.8757],\n",
      "        [0.7963, 0.8495, 0.8769, 0.8216, 0.8621, 0.8232, 1.0000, 0.8639, 0.8353,\n",
      "         0.8728, 0.8345, 0.8894, 0.8468, 0.8350, 0.8668, 0.7963, 0.8495, 0.8769,\n",
      "         0.8216, 0.8621, 0.8232, 0.8639, 0.8353, 0.8728, 0.8345, 0.8894, 0.8468,\n",
      "         0.8350, 0.8668],\n",
      "        [0.8248, 0.8287, 0.8923, 0.8514, 0.8733, 0.8735, 0.8639, 1.0000, 0.8923,\n",
      "         0.9073, 0.8325, 0.8360, 0.8141, 0.8703, 0.8655, 0.8248, 0.8287, 0.8923,\n",
      "         0.8514, 0.8733, 0.8735, 0.8639, 0.8923, 0.9073, 0.8325, 0.8360, 0.8141,\n",
      "         0.8703, 0.8655],\n",
      "        [0.7954, 0.7845, 0.8544, 0.8208, 0.8560, 0.8488, 0.8353, 0.8923, 1.0000,\n",
      "         0.8526, 0.7961, 0.8152, 0.8145, 0.8503, 0.8266, 0.7954, 0.7845, 0.8544,\n",
      "         0.8208, 0.8560, 0.8488, 0.8353, 0.8923, 0.8526, 0.7961, 0.8152, 0.8145,\n",
      "         0.8503, 0.8266],\n",
      "        [0.8657, 0.8557, 0.9001, 0.8700, 0.8877, 0.8874, 0.8728, 0.9073, 0.8526,\n",
      "         1.0000, 0.8161, 0.8547, 0.8297, 0.8906, 0.8599, 0.8657, 0.8557, 0.9001,\n",
      "         0.8700, 0.8877, 0.8874, 0.8728, 0.9073, 0.8526, 0.8161, 0.8547, 0.8297,\n",
      "         0.8906, 0.8599],\n",
      "        [0.8111, 0.8522, 0.9072, 0.8743, 0.8860, 0.8623, 0.8345, 0.8325, 0.7961,\n",
      "         0.8161, 1.0000, 0.8109, 0.8897, 0.8540, 0.8843, 0.8111, 0.8522, 0.9072,\n",
      "         0.8743, 0.8860, 0.8623, 0.8345, 0.8325, 0.7961, 0.8161, 0.8109, 0.8897,\n",
      "         0.8540, 0.8843],\n",
      "        [0.8016, 0.8757, 0.9003, 0.8419, 0.8420, 0.8479, 0.8894, 0.8360, 0.8152,\n",
      "         0.8547, 0.8109, 1.0000, 0.8603, 0.8776, 0.8545, 0.8016, 0.8757, 0.9003,\n",
      "         0.8419, 0.8420, 0.8479, 0.8894, 0.8360, 0.8152, 0.8547, 0.8109, 0.8603,\n",
      "         0.8776, 0.8545],\n",
      "        [0.8066, 0.8707, 0.8951, 0.8471, 0.8758, 0.8773, 0.8468, 0.8141, 0.8145,\n",
      "         0.8297, 0.8897, 0.8603, 1.0000, 0.8703, 0.8922, 0.8066, 0.8707, 0.8951,\n",
      "         0.8471, 0.8758, 0.8773, 0.8468, 0.8141, 0.8145, 0.8297, 0.8897, 0.8603,\n",
      "         0.8703, 0.8922],\n",
      "        [0.8522, 0.8549, 0.8972, 0.8542, 0.8639, 0.8830, 0.8350, 0.8703, 0.8503,\n",
      "         0.8906, 0.8540, 0.8776, 0.8703, 1.0000, 0.8779, 0.8522, 0.8549, 0.8972,\n",
      "         0.8542, 0.8639, 0.8830, 0.8350, 0.8703, 0.8503, 0.8906, 0.8540, 0.8776,\n",
      "         0.8703, 0.8779],\n",
      "        [0.8174, 0.8304, 0.8924, 0.8427, 0.8966, 0.8757, 0.8668, 0.8655, 0.8266,\n",
      "         0.8599, 0.8843, 0.8545, 0.8922, 0.8779, 1.0000, 0.8174, 0.8304, 0.8924,\n",
      "         0.8427, 0.8966, 0.8757, 0.8668, 0.8655, 0.8266, 0.8599, 0.8843, 0.8545,\n",
      "         0.8922, 0.8779]], grad_fn=<ViewBackward0>)\n",
      "labels:  torch.Size([30])\n",
      "tensor([14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,  0,  1,  2,\n",
      "         3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n"
     ]
    }
   ],
   "source": [
    "loss_fn = NTXentLoss(verbose=True)\n",
    "loss = loss_fn(z_i, z_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = NTXentLoss_chat()\n",
    "loss_2 = loss_fn(z_i, z_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0918, grad_fn=<NllLossBackward0>) tensor(3.0918, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss, loss_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_modules():\n",
    "    import importlib\n",
    "    import utils.image_processing as image_processing\n",
    "    import utils.file_IO as file_IO\n",
    "    import utils.visualization as visualization\n",
    "    import utils.tests as tests\n",
    "\n",
    "    importlib.reload(file_IO)\n",
    "    importlib.reload(image_processing)\n",
    "    importlib.reload(visualization)\n",
    "    importlib.reload(tests)\n",
    "\n",
    "    return image_processing, file_IO, visualization, tests\n",
    "image_processing, file_IO, visualization, tests = reload_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_batches(batch1, batch2, n=8, figsize=(16, 4)):\n",
    "    \"\"\"\n",
    "    Plots two batches of images: first row is batch1, second row is batch2.\n",
    "    Args:\n",
    "        batch1 (Tensor): Batch of images (B, C, H, W)\n",
    "        batch2 (Tensor): Batch of images (B, C, H, W)\n",
    "        n (int): Number of images to plot from each batch\n",
    "        figsize (tuple): Figure size\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    n = min(n, batch1.shape[0], batch2.shape[0])\n",
    "    fig, axes = plt.subplots(2, n, figsize=figsize)\n",
    "    memory=0\n",
    "    for i in range(n):\n",
    "        img1 = batch1[i].cpu()\n",
    "        img2 = batch2[i].cpu()\n",
    "        # Unnormalize if needed (assuming ImageNet stats)\n",
    "        '''if img1.shape[0] == 3:\n",
    "            img1 = img1 * torch.tensor([0.229, 0.224, 0.225]).view(3,1,1) + torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "            img2 = img2 * torch.tensor([0.229, 0.224, 0.225]).view(3,1,1) + torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "            img1 = img1.clamp(0,1)\n",
    "            img2 = img2.clamp(0,1)'''\n",
    "        axes[0, i].imshow(img1.permute(1, 2, 0).numpy())\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].imshow(img2.permute(1, 2, 0).numpy())\n",
    "        axes[1, i].axis('off')\n",
    "        memory += 2*tensor_memory_mb(img1)\n",
    "    axes[0, 0].set_ylabel('Batch 1', fontsize=14)\n",
    "    axes[1, 0].set_ylabel('Batch 2', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"Memory used for plotting: {memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_memory_mb(tensor):\n",
    "    \"\"\"Returns memory occupied by a torch tensor in megabytes (MB).\"\"\"\n",
    "    num_bytes = tensor.element_size() * tensor.nelement()\n",
    "    return num_bytes / (1024 ** 2)\n",
    "\n",
    "# Example usage:\n",
    "# memory_mb = tensor_memory_mb(batch['image1'])\n",
    "# print(f\"Memory occupied: {memory_mb:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeneralPurposeML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
