{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1123c1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\burtm\\Visual_studio_code\\conda_environments\\GeneralPurposeML\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(os.path.abspath(\"D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2add41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "\n",
    "# load image from the IAM database\n",
    "'''url = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")'''\n",
    "# Select one word image from the word_images list\n",
    "\n",
    "#processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\n",
    "#processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten',force_download=True) #force download clear the cache and redownload the model\n",
    "#processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten',use_fast=False)\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1',use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eee23cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.deit.modeling_deit.DeiTModel'> is overwritten by shared encoder config: DeiTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 384,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"deit\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.48.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 384,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 1024,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.48.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 64044\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd84d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "998f8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_utils.TruncatedDeiT(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "526255fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TruncatedDeiT(\n",
      "  (embeddings): DeiTEmbeddings(\n",
      "    (patch_embeddings): DeiTPatchEmbeddings(\n",
      "      (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): ModuleList(\n",
      "    (0-9): 10 x DeiTLayer(\n",
      "      (attention): DeiTSdpaAttention(\n",
      "        (attention): DeiTSdpaSelfAttention(\n",
      "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (output): DeiTSelfOutput(\n",
      "          (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): DeiTIntermediate(\n",
      "        (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): DeiTOutput(\n",
      "        (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "      (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f43944b",
   "metadata": {},
   "source": [
    "# standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec713cdf",
   "metadata": {},
   "source": [
    "how much time to fine tune the whole model? 60s per iteration -> 1h per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb1e68f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "N_max=282\n",
    "input_filename='icdar_train_df_cc_5patches_perName.csv'\n",
    "transform=processor\n",
    "huggingface=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c3eada1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m visualization\u001b[38;5;241m.\u001b[39mshow_single_image(\u001b[43mtrain_dataloader\u001b[49m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, from_dataloader\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "visualization.show_single_image(train_dataloader,index=1000, save_path=None, from_dataloader=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3b85963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is: \",device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "445a707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 images, out of 5640\n",
      "Processed 100 images, out of 5640\n",
      "Processed 200 images, out of 5640\n",
      "Processed 300 images, out of 5640\n",
      "Processed 400 images, out of 5640\n",
      "Processed 500 images, out of 5640\n",
      "Processed 600 images, out of 5640\n",
      "Processed 700 images, out of 5640\n",
      "Processed 800 images, out of 5640\n",
      "Processed 900 images, out of 5640\n",
      "Processed 1000 images, out of 5640\n",
      "Processed 1100 images, out of 5640\n",
      "Processed 1200 images, out of 5640\n",
      "Processed 1300 images, out of 5640\n",
      "Processed 1400 images, out of 5640\n",
      "Processed 1500 images, out of 5640\n",
      "Processed 1600 images, out of 5640\n",
      "Processed 1700 images, out of 5640\n",
      "Processed 1800 images, out of 5640\n",
      "Processed 1900 images, out of 5640\n",
      "Processed 2000 images, out of 5640\n",
      "Processed 2100 images, out of 5640\n",
      "Processed 2200 images, out of 5640\n",
      "Processed 2300 images, out of 5640\n",
      "Processed 2400 images, out of 5640\n",
      "Processed 2500 images, out of 5640\n",
      "Processed 2600 images, out of 5640\n",
      "Processed 2700 images, out of 5640\n",
      "Processed 2800 images, out of 5640\n",
      "Processed 2900 images, out of 5640\n",
      "Processed 3000 images, out of 5640\n",
      "Processed 3100 images, out of 5640\n",
      "Processed 3200 images, out of 5640\n",
      "Processed 3300 images, out of 5640\n",
      "Processed 3400 images, out of 5640\n",
      "Processed 3500 images, out of 5640\n",
      "Processed 3600 images, out of 5640\n",
      "Processed 3700 images, out of 5640\n",
      "Processed 3800 images, out of 5640\n",
      "Processed 3900 images, out of 5640\n",
      "Processed 4000 images, out of 5640\n",
      "Processed 4100 images, out of 5640\n",
      "Processed 4200 images, out of 5640\n",
      "Processed 4300 images, out of 5640\n",
      "Processed 4400 images, out of 5640\n",
      "Processed 4500 images, out of 5640\n",
      "Processed 4600 images, out of 5640\n",
      "Processed 4700 images, out of 5640\n",
      "Processed 4800 images, out of 5640\n",
      "Processed 4900 images, out of 5640\n",
      "Processed 5000 images, out of 5640\n",
      "Processed 5100 images, out of 5640\n",
      "Processed 5200 images, out of 5640\n",
      "Processed 5300 images, out of 5640\n",
      "Processed 5400 images, out of 5640\n",
      "Processed 5500 images, out of 5640\n",
      "Processed 5600 images, out of 5640\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "source_path=\"D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\"\n",
    "train_df = pd.read_csv(f\"{source_path}\\\\outputs\\\\preprocessed_data\\\\{input_filename}\")\n",
    "model.eval()\n",
    "\n",
    "# Initialize a dictionary to store new feature columns\n",
    "new_features = {}\n",
    "\n",
    "for index,t in train_df.iterrows():\n",
    "    image_file = t['file_name']\n",
    "    x1 = t['x']\n",
    "    y1 = t['y']\n",
    "    x2 = t['x2']\n",
    "    y2 = t['y2']\n",
    "    image = Image.open(image_file).convert(\"RGB\")\n",
    "    patch = image.crop((x1, y1, x2, y2))\n",
    "    if huggingface:\n",
    "        # the transform is actually an huggingface processor in this case\n",
    "        inputs = transform(images=patch, return_tensors=\"pt\")\n",
    "        # Remove batch dimension from inputs\n",
    "        patch = inputs['pixel_values'].squeeze()\n",
    "    elif transform:\n",
    "        patch = transform(patch)\n",
    "    patch = patch.to(device)\n",
    "    output = model(patch.unsqueeze(0))\n",
    "    #cls_token_output = output.last_hidden_state[:, 0, :]  # Extract the CLS token\n",
    "    cls_token_output = output[:, 0, :] #if i am using a truncated model there is no last_hidden_state argument\n",
    "    # Add CLS token output dimensions as new columns to the dataframe\n",
    "    for i, value in enumerate(cls_token_output.squeeze().tolist()):\n",
    "        column_name = f\"f{i+1}\"\n",
    "        if column_name not in new_features:\n",
    "            new_features[column_name] = []\n",
    "        new_features[column_name].append(value)\n",
    "    if index % 100 == 0:\n",
    "        print(f\"Processed {index} images, out of {len(train_df)}\")\n",
    "\n",
    "    \n",
    "# Add the new features to the DataFrame in one operation\n",
    "new_features_df = pd.DataFrame(new_features)\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), new_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11439c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 images, out of 5640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.dataframes import CustomImageDataset, CustomPatchDataset\n",
    "from torch.utils.data import Dataset,DataLoader, random_split\n",
    "source_path=\"D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\"\n",
    "train_df = pd.read_csv(f\"{source_path}\\\\outputs\\\\preprocessed_data\\\\{input_filename}\")\n",
    "model.eval()\n",
    "dataset = CustomPatchDataset(train_df[train_df['writer']<=N_max] ,\n",
    "                                        label_column='male', transform=transform, huggingface=huggingface)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "new_features = {}\n",
    "\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    patch = batch['image'].to(device)\n",
    "    output = model(pixel_values=patch)\n",
    "    cls_token_output = output.last_hidden_state[:, 0, :]  # Extract the CLS token\n",
    "    for i, value in enumerate(cls_token_output.squeeze().tolist()):\n",
    "        column_name = f\"f{i+1}\"\n",
    "        if column_name not in new_features:\n",
    "            new_features[column_name] = []\n",
    "        new_features[column_name].append(value)\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx * batch_size} images, out of {len(dataset)}\")\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "new_features_df = pd.DataFrame(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b1a3b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>writer</th>\n",
       "      <th>same_text</th>\n",
       "      <th>isEng</th>\n",
       "      <th>train</th>\n",
       "      <th>file_name</th>\n",
       "      <th>male</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>...</th>\n",
       "      <th>f375</th>\n",
       "      <th>f376</th>\n",
       "      <th>f377</th>\n",
       "      <th>f378</th>\n",
       "      <th>f379</th>\n",
       "      <th>f380</th>\n",
       "      <th>f381</th>\n",
       "      <th>f382</th>\n",
       "      <th>f383</th>\n",
       "      <th>f384</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>D:\\download\\PD project\\datasets\\ICDAR 2013 - G...</td>\n",
       "      <td>1</td>\n",
       "      <td>988</td>\n",
       "      <td>494</td>\n",
       "      <td>1482</td>\n",
       "      <td>988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.922357</td>\n",
       "      <td>2.522360</td>\n",
       "      <td>4.304227</td>\n",
       "      <td>-3.425128</td>\n",
       "      <td>3.495620</td>\n",
       "      <td>-4.153519</td>\n",
       "      <td>-0.852054</td>\n",
       "      <td>-1.164748</td>\n",
       "      <td>-2.234782</td>\n",
       "      <td>-2.404248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D:\\download\\PD project\\datasets\\ICDAR 2013 - G...</td>\n",
       "      <td>0</td>\n",
       "      <td>990</td>\n",
       "      <td>495</td>\n",
       "      <td>1485</td>\n",
       "      <td>990</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.485612</td>\n",
       "      <td>1.888572</td>\n",
       "      <td>4.129086</td>\n",
       "      <td>-3.297427</td>\n",
       "      <td>4.332079</td>\n",
       "      <td>-4.687369</td>\n",
       "      <td>-2.287635</td>\n",
       "      <td>-1.905988</td>\n",
       "      <td>-4.285491</td>\n",
       "      <td>-1.362528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D:\\download\\PD project\\datasets\\ICDAR 2013 - G...</td>\n",
       "      <td>1</td>\n",
       "      <td>494</td>\n",
       "      <td>1976</td>\n",
       "      <td>988</td>\n",
       "      <td>2470</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.275970</td>\n",
       "      <td>1.497854</td>\n",
       "      <td>3.086382</td>\n",
       "      <td>-2.358793</td>\n",
       "      <td>4.704427</td>\n",
       "      <td>-5.747137</td>\n",
       "      <td>-3.386876</td>\n",
       "      <td>1.461058</td>\n",
       "      <td>-3.862585</td>\n",
       "      <td>-1.632300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>D:\\download\\PD project\\datasets\\ICDAR 2013 - G...</td>\n",
       "      <td>1</td>\n",
       "      <td>494</td>\n",
       "      <td>494</td>\n",
       "      <td>988</td>\n",
       "      <td>988</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.605459</td>\n",
       "      <td>2.483239</td>\n",
       "      <td>4.245601</td>\n",
       "      <td>-3.242137</td>\n",
       "      <td>3.766826</td>\n",
       "      <td>-4.104949</td>\n",
       "      <td>-1.693988</td>\n",
       "      <td>-1.805966</td>\n",
       "      <td>-2.415438</td>\n",
       "      <td>-1.441694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D:\\download\\PD project\\datasets\\ICDAR 2013 - G...</td>\n",
       "      <td>0</td>\n",
       "      <td>1482</td>\n",
       "      <td>494</td>\n",
       "      <td>1976</td>\n",
       "      <td>988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.307438</td>\n",
       "      <td>1.057468</td>\n",
       "      <td>2.697207</td>\n",
       "      <td>-1.516698</td>\n",
       "      <td>3.344037</td>\n",
       "      <td>-4.181660</td>\n",
       "      <td>-2.132452</td>\n",
       "      <td>-1.470941</td>\n",
       "      <td>-4.696310</td>\n",
       "      <td>-0.665148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 395 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   writer  same_text  isEng  train  \\\n",
       "0     190          0      0      1   \n",
       "1      24          0      1      1   \n",
       "2      26          1      1      1   \n",
       "3     190          0      0      1   \n",
       "4     160          0      1      1   \n",
       "\n",
       "                                           file_name  male     x     y    x2  \\\n",
       "0  D:\\download\\PD project\\datasets\\ICDAR 2013 - G...     1   988   494  1482   \n",
       "1  D:\\download\\PD project\\datasets\\ICDAR 2013 - G...     0   990   495  1485   \n",
       "2  D:\\download\\PD project\\datasets\\ICDAR 2013 - G...     1   494  1976   988   \n",
       "3  D:\\download\\PD project\\datasets\\ICDAR 2013 - G...     1   494   494   988   \n",
       "4  D:\\download\\PD project\\datasets\\ICDAR 2013 - G...     0  1482   494  1976   \n",
       "\n",
       "     y2  ...      f375      f376      f377      f378      f379      f380  \\\n",
       "0   988  ... -0.922357  2.522360  4.304227 -3.425128  3.495620 -4.153519   \n",
       "1   990  ... -0.485612  1.888572  4.129086 -3.297427  4.332079 -4.687369   \n",
       "2  2470  ... -1.275970  1.497854  3.086382 -2.358793  4.704427 -5.747137   \n",
       "3   988  ... -1.605459  2.483239  4.245601 -3.242137  3.766826 -4.104949   \n",
       "4   988  ... -0.307438  1.057468  2.697207 -1.516698  3.344037 -4.181660   \n",
       "\n",
       "       f381      f382      f383      f384  \n",
       "0 -0.852054 -1.164748 -2.234782 -2.404248  \n",
       "1 -2.287635 -1.905988 -4.285491 -1.362528  \n",
       "2 -3.386876  1.461058 -3.862585 -1.632300  \n",
       "3 -1.693988 -1.805966 -2.415438 -1.441694  \n",
       "4 -2.132452 -1.470941 -4.696310 -0.665148  \n",
       "\n",
       "[5 rows x 395 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd41e5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: D:\\burtm\\Visual_studio_code\\PD_related_projects\\outputs\\preprocessed_data\\icdar_EXTRACTED_trocr_stage_1_10blocks_20250509_153553.csv\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "file_path=source_path+f\"\\\\outputs\\\\preprocessed_data\\\\icdar_EXTRACTED_trocr_stage_1_10blocks_{timestamp}.csv\"\n",
    "train_df.to_csv(file_path, index=False)\n",
    "print(f\"File saved: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2dfb57",
   "metadata": {},
   "source": [
    "# easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e39392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_modules():\n",
    "    import importlib\n",
    "    import utils.data_loading as data_loading\n",
    "    import utils.visualization as visualization\n",
    "    import utils.dataframes as dataframes\n",
    "    import utils.utils_transforms as u_transforms\n",
    "    import utils.training_utils as training_utils\n",
    "    import utils.model_utils as model_utils\n",
    "    \n",
    "\n",
    "    importlib.reload(data_loading)\n",
    "    importlib.reload(visualization)\n",
    "    importlib.reload(dataframes)\n",
    "    importlib.reload(u_transforms)\n",
    "    importlib.reload(model_utils)\n",
    "    importlib.reload(training_utils)\n",
    "\n",
    "    return data_loading, visualization, dataframes, u_transforms, training_utils, model_utils\n",
    "data_loading, visualization, dataframes, u_transforms, training_utils, model_utils = reload_modules()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeneralPurposeML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
