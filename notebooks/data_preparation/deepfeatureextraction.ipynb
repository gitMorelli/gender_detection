{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1123c1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\burtm\\Visual_studio_code\\conda_environments\\GeneralPurposeML\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(os.path.abspath(\"D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a2add41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.deit.modeling_deit.DeiTModel'> is overwritten by shared encoder config: DeiTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 384,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"deit\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.48.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 384,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 1024,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.48.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 64044\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "selected_model = 'trocr-small-stage1'\n",
    "\n",
    "transform = u_transforms.get_transform(selected_model)\n",
    "model = model_utils.get_model(name=selected_model, mode='truncated', pretrained=True, truncation='remove head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4494fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_max=282\n",
    "patches=True\n",
    "input_filename='icdar_train_df_patches_20250515_164130.csv'\n",
    "huggingface=True\n",
    "pooling=False # if true in transformer mdoels use pooling, if false only the cls token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3b85963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is: \",device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "342c1500",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path=\"D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\"\n",
    "train_df = pd.read_csv(f\"{source_path}\\\\outputs\\\\preprocessed_data\\\\{input_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bd7fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "output=compute_output(model, device, transform, train_df.iloc[i], huggingface, patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "445a707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 images, out of 5640\n",
      "Processed 100 images, out of 5640\n",
      "Processed 200 images, out of 5640\n",
      "Processed 300 images, out of 5640\n",
      "Processed 400 images, out of 5640\n",
      "Processed 500 images, out of 5640\n",
      "Processed 600 images, out of 5640\n",
      "Processed 700 images, out of 5640\n",
      "Processed 800 images, out of 5640\n",
      "Processed 900 images, out of 5640\n",
      "Processed 1000 images, out of 5640\n",
      "Processed 1100 images, out of 5640\n",
      "Processed 1200 images, out of 5640\n",
      "Processed 1300 images, out of 5640\n",
      "Processed 1400 images, out of 5640\n",
      "Processed 1500 images, out of 5640\n",
      "Processed 1600 images, out of 5640\n",
      "Processed 1700 images, out of 5640\n",
      "Processed 1800 images, out of 5640\n",
      "Processed 1900 images, out of 5640\n",
      "Processed 2000 images, out of 5640\n",
      "Processed 2100 images, out of 5640\n",
      "Processed 2200 images, out of 5640\n",
      "Processed 2300 images, out of 5640\n",
      "Processed 2400 images, out of 5640\n",
      "Processed 2500 images, out of 5640\n",
      "Processed 2600 images, out of 5640\n",
      "Processed 2700 images, out of 5640\n",
      "Processed 2800 images, out of 5640\n",
      "Processed 2900 images, out of 5640\n",
      "Processed 3000 images, out of 5640\n",
      "Processed 3100 images, out of 5640\n",
      "Processed 3200 images, out of 5640\n",
      "Processed 3300 images, out of 5640\n",
      "Processed 3400 images, out of 5640\n",
      "Processed 3500 images, out of 5640\n",
      "Processed 3600 images, out of 5640\n",
      "Processed 3700 images, out of 5640\n",
      "Processed 3800 images, out of 5640\n",
      "Processed 3900 images, out of 5640\n",
      "Processed 4000 images, out of 5640\n",
      "Processed 4100 images, out of 5640\n",
      "Processed 4200 images, out of 5640\n",
      "Processed 4300 images, out of 5640\n",
      "Processed 4400 images, out of 5640\n",
      "Processed 4500 images, out of 5640\n",
      "Processed 4600 images, out of 5640\n",
      "Processed 4700 images, out of 5640\n",
      "Processed 4800 images, out of 5640\n",
      "Processed 4900 images, out of 5640\n",
      "Processed 5000 images, out of 5640\n",
      "Processed 5100 images, out of 5640\n",
      "Processed 5200 images, out of 5640\n",
      "Processed 5300 images, out of 5640\n",
      "Processed 5400 images, out of 5640\n",
      "Processed 5500 images, out of 5640\n",
      "Processed 5600 images, out of 5640\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Initialize a dictionary to store new feature columns\n",
    "new_features = {}\n",
    "\n",
    "for index,t in train_df.iterrows():\n",
    "    if pooling:\n",
    "        print(\"Pooling is not implemented yet\")\n",
    "        break\n",
    "    else:\n",
    "        output = compute_output(model, device, transform, t, huggingface, patches)[:,0,:]\n",
    "    for i, value in enumerate(output.squeeze().tolist()):\n",
    "        column_name = f\"f{i+1}\"\n",
    "        if column_name not in new_features:\n",
    "            new_features[column_name] = []\n",
    "        new_features[column_name].append(value)\n",
    "    if index % 100 == 0:\n",
    "        print(f\"Processed {index} images, out of {len(train_df)}\")\n",
    "\n",
    "    \n",
    "# Add the new features to the DataFrame in one operation\n",
    "new_features_df = pd.DataFrame(new_features)\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), new_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4c38b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>writer</th>\n",
       "      <th>isEng</th>\n",
       "      <th>same_text</th>\n",
       "      <th>file_name</th>\n",
       "      <th>male</th>\n",
       "      <th>train</th>\n",
       "      <th>index</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x2</th>\n",
       "      <th>...</th>\n",
       "      <th>f375</th>\n",
       "      <th>f376</th>\n",
       "      <th>f377</th>\n",
       "      <th>f378</th>\n",
       "      <th>f379</th>\n",
       "      <th>f380</th>\n",
       "      <th>f381</th>\n",
       "      <th>f382</th>\n",
       "      <th>f383</th>\n",
       "      <th>f384</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D:\\download\\PD project\\datasets\\ICDAR 2013 - G...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.622814</td>\n",
       "      <td>0.652870</td>\n",
       "      <td>-0.036768</td>\n",
       "      <td>-0.072947</td>\n",
       "      <td>1.055691</td>\n",
       "      <td>-0.420997</td>\n",
       "      <td>0.843602</td>\n",
       "      <td>-0.768733</td>\n",
       "      <td>0.928809</td>\n",
       "      <td>-0.653204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D:\\download\\PD project\\datasets\\ICDAR 2013 - G...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466590</td>\n",
       "      <td>0.390466</td>\n",
       "      <td>-0.063519</td>\n",
       "      <td>-0.098352</td>\n",
       "      <td>0.813783</td>\n",
       "      <td>-0.317575</td>\n",
       "      <td>0.831497</td>\n",
       "      <td>-0.474965</td>\n",
       "      <td>0.726222</td>\n",
       "      <td>-0.933849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D:\\download\\PD project\\datasets\\ICDAR 2013 - G...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1479</td>\n",
       "      <td>493</td>\n",
       "      <td>1972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.492431</td>\n",
       "      <td>0.539464</td>\n",
       "      <td>0.355833</td>\n",
       "      <td>-0.734787</td>\n",
       "      <td>1.332562</td>\n",
       "      <td>-0.239772</td>\n",
       "      <td>0.353327</td>\n",
       "      <td>-0.422959</td>\n",
       "      <td>0.952368</td>\n",
       "      <td>-0.632744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D:\\download\\PD project\\datasets\\ICDAR 2013 - G...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>986</td>\n",
       "      <td>493</td>\n",
       "      <td>1479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411978</td>\n",
       "      <td>0.623444</td>\n",
       "      <td>0.031737</td>\n",
       "      <td>0.048326</td>\n",
       "      <td>0.664006</td>\n",
       "      <td>-0.226131</td>\n",
       "      <td>0.853005</td>\n",
       "      <td>-0.710104</td>\n",
       "      <td>1.464373</td>\n",
       "      <td>-0.562862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D:\\download\\PD project\\datasets\\ICDAR 2013 - G...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1972</td>\n",
       "      <td>493</td>\n",
       "      <td>2465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963354</td>\n",
       "      <td>0.558444</td>\n",
       "      <td>-0.039232</td>\n",
       "      <td>0.150385</td>\n",
       "      <td>1.289550</td>\n",
       "      <td>-0.446754</td>\n",
       "      <td>0.663165</td>\n",
       "      <td>-0.859034</td>\n",
       "      <td>1.164699</td>\n",
       "      <td>-0.655603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 396 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   writer  isEng  same_text  \\\n",
       "0       1      0          0   \n",
       "1       1      0          0   \n",
       "2       1      0          0   \n",
       "3       1      0          0   \n",
       "4       1      0          0   \n",
       "\n",
       "                                           file_name  male  train  index  \\\n",
       "0  D:\\download\\PD project\\datasets\\ICDAR 2013 - G...     0      1      0   \n",
       "1  D:\\download\\PD project\\datasets\\ICDAR 2013 - G...     0      1      1   \n",
       "2  D:\\download\\PD project\\datasets\\ICDAR 2013 - G...     0      1      2   \n",
       "3  D:\\download\\PD project\\datasets\\ICDAR 2013 - G...     0      1      3   \n",
       "4  D:\\download\\PD project\\datasets\\ICDAR 2013 - G...     0      1      4   \n",
       "\n",
       "      x    y    x2  ...      f375      f376      f377      f378      f379  \\\n",
       "0     0  493   493  ...  0.622814  0.652870 -0.036768 -0.072947  1.055691   \n",
       "1   493  493   986  ...  0.466590  0.390466 -0.063519 -0.098352  0.813783   \n",
       "2  1479  493  1972  ...  0.492431  0.539464  0.355833 -0.734787  1.332562   \n",
       "3   986  493  1479  ...  0.411978  0.623444  0.031737  0.048326  0.664006   \n",
       "4  1972  493  2465  ...  0.963354  0.558444 -0.039232  0.150385  1.289550   \n",
       "\n",
       "       f380      f381      f382      f383      f384  \n",
       "0 -0.420997  0.843602 -0.768733  0.928809 -0.653204  \n",
       "1 -0.317575  0.831497 -0.474965  0.726222 -0.933849  \n",
       "2 -0.239772  0.353327 -0.422959  0.952368 -0.632744  \n",
       "3 -0.226131  0.853005 -0.710104  1.464373 -0.562862  \n",
       "4 -0.446754  0.663165 -0.859034  1.164699 -0.655603  \n",
       "\n",
       "[5 rows x 396 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11439c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 images, out of 5640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''from utils.dataframes import CustomImageDataset, CustomPatchDataset\n",
    "from torch.utils.data import Dataset,DataLoader, random_split\n",
    "source_path=\"D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\"\n",
    "train_df = pd.read_csv(f\"{source_path}\\\\outputs\\\\preprocessed_data\\\\{input_filename}\")\n",
    "model.eval()\n",
    "dataset = CustomPatchDataset(train_df[train_df['writer']<=N_max] ,\n",
    "                                        label_column='male', transform=transform, huggingface=huggingface)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "new_features = {}\n",
    "\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    patch = batch['image'].to(device)\n",
    "    output = model(pixel_values=patch)\n",
    "    cls_token_output = output.last_hidden_state[:, 0, :]  # Extract the CLS token\n",
    "    for i, value in enumerate(cls_token_output.squeeze().tolist()):\n",
    "        column_name = f\"f{i+1}\"\n",
    "        if column_name not in new_features:\n",
    "            new_features[column_name] = []\n",
    "        new_features[column_name].append(value)\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx * batch_size} images, out of {len(dataset)}\")\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "new_features_df = pd.DataFrame(new_features)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4647abe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to D:\\burtm\\Visual_studio_code\\PD_related_projects\\outputs\\preprocessed_data\\icdar_EXTRACTED_train_df_trocr-small-stage1_20250516_122528.csv\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = os.path.join(source_path, \"outputs\", \"preprocessed_data\")\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "output_file = os.path.join(output_dir, f\"icdar_EXTRACTED_train_df_{selected_model}_{timestamp}.csv\")\n",
    "train_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Dataframe saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f03398c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file path: D:\\burtm\\Visual_studio_code\\PD_related_projects\\outputs\\preprocessed_data\\file_metadata_log.json\n",
      "Output file path: D:\\burtm\\Visual_studio_code\\PD_related_projects\\outputs\\preprocessed_data\\icdar_EXTRACTED_train_df_trocr-small-stage1_20250516_122528.csv\n",
      "Updated log for icdar_EXTRACTED_train_df_trocr-small-stage1_20250516_122528.csv\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "LOG_FILE = output_dir+\"\\\\file_metadata_log.json\"\n",
    "print(f\"Log file path: {LOG_FILE}\")\n",
    "print(f\"Output file path: {output_file}\")\n",
    "file_IO.add_or_update_file(\n",
    "    output_file, LOG_FILE,\n",
    "    custom_metadata={\n",
    "        #\"seed\": seed,\n",
    "        \"source_file\": input_filename,\n",
    "        \"model\": selected_model,\n",
    "        \"pooling\": pooling,\n",
    "        \"description\": '''I am performing inference with trocr-small-stage1 on the patches dataset and saving the output features as columns of the dataframe\n",
    "        the columns are named f1, f2, .., f384''' \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd009b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for icdar_EXTRACTED_train_df_trocr-small-stage1_20250516_122528.csv:\n",
      "full_path: D:\\burtm\\Visual_studio_code\\PD_related_projects\\outputs\\preprocessed_data\\icdar_EXTRACTED_train_df_trocr-small-stage1_20250516_122528.csv\n",
      "size_bytes: 43379066\n",
      "created: 2025-05-16T12:25:28.589676\n",
      "modified: 2025-05-16T12:25:33.533804\n",
      "accessed: 2025-05-16T12:25:33.533804\n",
      "source_file: icdar_train_df_patches_20250515_164130.csv\n",
      "model: trocr-small-stage1\n",
      "pooling: False\n",
      "description: I am performing inference with trocr-small-stage1 on the patches dataset and saving the output features as columns of the dataframe\n",
      "        the columns are named f1, f2, .., f384\n"
     ]
    }
   ],
   "source": [
    "file_IO.read_metadata(\n",
    "    output_file,\n",
    "    log_path=LOG_FILE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd41e5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: D:\\burtm\\Visual_studio_code\\PD_related_projects\\outputs\\preprocessed_data\\icdar_EXTRACTED_trocr_stage_1_10blocks_20250509_153553.csv\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "file_path=source_path+f\"\\\\outputs\\\\preprocessed_data\\\\icdar_EXTRACTED_trocr_stage_1_10blocks_{timestamp}.csv\"\n",
    "train_df.to_csv(file_path, index=False)\n",
    "print(f\"File saved: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14cff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b2dfb57",
   "metadata": {},
   "source": [
    "# easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82e39392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_modules():\n",
    "    import importlib\n",
    "    import utils.data_loading as data_loading\n",
    "    import utils.visualization as visualization\n",
    "    import utils.dataframes as dataframes\n",
    "    import utils.utils_transforms as u_transforms\n",
    "    import utils.training_utils as training_utils\n",
    "    import utils.model_utils as model_utils\n",
    "    import utils.file_IO as file_IO\n",
    "    \n",
    "    importlib.reload(file_IO)\n",
    "    importlib.reload(data_loading)\n",
    "    importlib.reload(visualization)\n",
    "    importlib.reload(dataframes)\n",
    "    importlib.reload(u_transforms)\n",
    "    importlib.reload(model_utils)\n",
    "    importlib.reload(training_utils)\n",
    "\n",
    "    return data_loading, visualization, dataframes, u_transforms, training_utils, model_utils, file_IO\n",
    "data_loading, visualization, dataframes, u_transforms, training_utils, model_utils, file_IO = reload_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "807c5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output(model, device, transform, t, huggingface, patches):\n",
    "    image_file = t['file_name']\n",
    "    image = Image.open(image_file).convert(\"RGB\")\n",
    "    if patches:\n",
    "        x1 = t['x']\n",
    "        y1 = t['y']\n",
    "        x2 = t['x2']\n",
    "        y2 = t['y2']\n",
    "        patch = image.crop((x1, y1, x2, y2))\n",
    "    else:\n",
    "        patch = image.copy()\n",
    "    if huggingface:\n",
    "        # the transform is actually an huggingface processor in this case\n",
    "        inputs = transform(images=patch, return_tensors=\"pt\")\n",
    "        # Remove batch dimension from inputs\n",
    "        patch = inputs['pixel_values'].squeeze()\n",
    "    else:\n",
    "        patch = transform(patch)\n",
    "    patch = patch.to(device)\n",
    "    output = model(patch.unsqueeze(0))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfdbc167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(transform, t, huggingface, patches):\n",
    "    image_file = t['file_name']\n",
    "    image = Image.open(image_file).convert(\"RGB\")\n",
    "    if patches:\n",
    "        x1 = t['x']\n",
    "        y1 = t['y']\n",
    "        x2 = t['x2']\n",
    "        y2 = t['y2']\n",
    "        patch = image.crop((x1, y1, x2, y2))\n",
    "    else:\n",
    "        patch = image.copy()\n",
    "\n",
    "    # Show original image/patch\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(patch)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    if huggingface:\n",
    "        inputs = transform(images=patch, return_tensors=\"pt\")\n",
    "        transformed_patch = inputs['pixel_values'].squeeze()\n",
    "        # Convert tensor to numpy for visualization\n",
    "    else:\n",
    "        transformed_patch = transform(patch)\n",
    "    \n",
    "    img_np = transformed_patch.permute(1, 2, 0).cpu().numpy()\n",
    "    # Normalize if needed\n",
    "    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img_np)\n",
    "    plt.title(\"Transformed\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeneralPurposeML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
