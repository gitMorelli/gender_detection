{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1123c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(os.path.abspath(\"D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef851347",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_max=282\n",
    "patches=True\n",
    "input_filename='icdar_train_df_patches_20250515_164130.csv'\n",
    "huggingface=False\n",
    "pooling=False # if true in transformer mdoels use pooling, if false only the cls token\n",
    "custom_transform=True\n",
    "transform_mode='padding'\n",
    "save_h5=False\n",
    "selected_model = 'crnn_vgg16_bn'\n",
    "truncation = 'remove head'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a2add41",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = u_transforms.get_transform(selected_model, use_patches=patches, custom=custom_transform, mode=transform_mode)\n",
    "model = model_utils.get_model(name=selected_model, mode='truncated', pretrained=True, truncation=truncation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3b85963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is: \",device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "342c1500",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path=\"D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\"\n",
    "train_df = pd.read_csv(f\"{source_path}\\\\outputs\\\\preprocessed_data\\\\{input_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89e74c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualization.show_preprocessed_from_csv(train_df,transform,hugging=huggingface,index=0,patches=patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f0567a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import numpy as np\\nindex=0\\n# Select an image row from train_df\\nsample_row = train_df.iloc[index]\\nimage_file = sample_row[\\'file_name\\']\\nimage = Image.open(image_file).convert(\"RGB\")\\n\\n# If using patches, crop the patch, else use the whole image\\nif patches:\\n    x1, y1, x2, y2 = sample_row[\\'x\\'], sample_row[\\'y\\'], sample_row[\\'x2\\'], sample_row[\\'y2\\']\\n    patch = image.crop((x1, y1, x2, y2))\\nelse:\\n    patch = image.copy()\\n\\npatch=np.array(patch)\\npatch_list=[patch]\\nfrom doctr.models.preprocessor import PreProcessor\\ntransform=PreProcessor(\\n        (32, 128),\\n        batch_size=1,\\n        mean=(0.798, 0.785, 0.772),\\n        std=(0.264, 0.2749, 0.287),\\n        #preserve_aspect_ratio=True,\\n        #symmetric_pad=True,\\n    )\\npatch=transform(patch_list)\\npatch = patch[0].squeeze(0)\\nimg_np = patch.permute(1, 2, 0).cpu().numpy()\\nimg_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\\n\\nplt.imshow(img_np)\\nplt.title(\"Preprocessed Image\")\\nplt.axis(\\'off\\')\\nplt.show()'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dctocr test\n",
    "\n",
    "import numpy as np\n",
    "index=0\n",
    "# Select an image row from train_df\n",
    "sample_row = train_df.iloc[index]\n",
    "image_file = sample_row['file_name']\n",
    "image = Image.open(image_file).convert(\"RGB\")\n",
    "\n",
    "# If using patches, crop the patch, else use the whole image\n",
    "if patches:\n",
    "    x1, y1, x2, y2 = sample_row['x'], sample_row['y'], sample_row['x2'], sample_row['y2']\n",
    "    patch = image.crop((x1, y1, x2, y2))\n",
    "else:\n",
    "    patch = image.copy()\n",
    "\n",
    "patch=np.array(patch)\n",
    "patch_list=[patch]\n",
    "from doctr.models.preprocessor import PreProcessor\n",
    "transform=PreProcessor(\n",
    "        (32, 128),\n",
    "        batch_size=1,\n",
    "        mean=(0.798, 0.785, 0.772),\n",
    "        std=(0.264, 0.2749, 0.287),\n",
    "        #preserve_aspect_ratio=True,\n",
    "        #symmetric_pad=True,\n",
    "    )\n",
    "patch=transform(patch_list)\n",
    "patch = patch[0].squeeze(0)\n",
    "img_np = patch.permute(1, 2, 0).cpu().numpy()\n",
    "img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "\n",
    "plt.imshow(img_np)\n",
    "plt.title(\"Preprocessed Image\")\n",
    "plt.axis('off')\n",
    "plt.show()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0bd7fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "output=compute_output(model, device, transform, train_df.iloc[i], huggingface, patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "747a4420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"Output shape: \", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "445a707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 images, out of 5640\n",
      "Processed 100 images, out of 5640\n",
      "Processed 200 images, out of 5640\n",
      "Processed 300 images, out of 5640\n",
      "Processed 400 images, out of 5640\n",
      "Processed 500 images, out of 5640\n",
      "Processed 600 images, out of 5640\n",
      "Processed 700 images, out of 5640\n",
      "Processed 800 images, out of 5640\n",
      "Processed 900 images, out of 5640\n",
      "Processed 1000 images, out of 5640\n",
      "Processed 1100 images, out of 5640\n",
      "Processed 1200 images, out of 5640\n",
      "Processed 1300 images, out of 5640\n",
      "Processed 1400 images, out of 5640\n",
      "Processed 1500 images, out of 5640\n",
      "Processed 1600 images, out of 5640\n",
      "Processed 1700 images, out of 5640\n",
      "Processed 1800 images, out of 5640\n",
      "Processed 1900 images, out of 5640\n",
      "Processed 2000 images, out of 5640\n",
      "Processed 2100 images, out of 5640\n",
      "Processed 2200 images, out of 5640\n",
      "Processed 2300 images, out of 5640\n",
      "Processed 2400 images, out of 5640\n",
      "Processed 2500 images, out of 5640\n",
      "Processed 2600 images, out of 5640\n",
      "Processed 2700 images, out of 5640\n",
      "Processed 2800 images, out of 5640\n",
      "Processed 2900 images, out of 5640\n",
      "Processed 3000 images, out of 5640\n",
      "Processed 3100 images, out of 5640\n",
      "Processed 3200 images, out of 5640\n",
      "Processed 3300 images, out of 5640\n",
      "Processed 3400 images, out of 5640\n",
      "Processed 3500 images, out of 5640\n",
      "Processed 3600 images, out of 5640\n",
      "Processed 3700 images, out of 5640\n",
      "Processed 3800 images, out of 5640\n",
      "Processed 3900 images, out of 5640\n",
      "Processed 4000 images, out of 5640\n",
      "Processed 4100 images, out of 5640\n",
      "Processed 4200 images, out of 5640\n",
      "Processed 4300 images, out of 5640\n",
      "Processed 4400 images, out of 5640\n",
      "Processed 4500 images, out of 5640\n",
      "Processed 4600 images, out of 5640\n",
      "Processed 4700 images, out of 5640\n",
      "Processed 4800 images, out of 5640\n",
      "Processed 4900 images, out of 5640\n",
      "Processed 5000 images, out of 5640\n",
      "Processed 5100 images, out of 5640\n",
      "Processed 5200 images, out of 5640\n",
      "Processed 5300 images, out of 5640\n",
      "Processed 5400 images, out of 5640\n",
      "Processed 5500 images, out of 5640\n",
      "Processed 5600 images, out of 5640\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "if save_h5:\n",
    "    import h5py\n",
    "    import numpy as np\n",
    "    import shutil\n",
    "    icdar_path=\"D:\\\\download\\\\PD project\\\\datasets\\\\ICDAR 2013 - Gender Identification Competition Dataset\\\\\"\n",
    "    # Define the directory and file paths\n",
    "    h5_directory_name = \"extracted_representations_full\"\n",
    "    h5_save_path=icdar_path+h5_directory_name\n",
    "    # Open the file in append mode\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    h5_file_name = h5_save_path+f\"\\\\representations_{selected_model}_{truncation}_{timestamp}.h5\"\n",
    "    with h5py.File(h5_file_name, \"a\") as f:\n",
    "        model.eval()\n",
    "        for index,t in train_df.iterrows():\n",
    "            if huggingface:\n",
    "                output = compute_output(model, device, transform, t, huggingface, patches)\n",
    "            else:\n",
    "                output = compute_output(model, device, transform, t, huggingface, patches)\n",
    "            #print(output)\n",
    "            # Convert index to string key (e.g., \"0001\")\n",
    "            key = f\"{index:06d}\"\n",
    "            # Store with compression (optional)\n",
    "            rep_np = output.squeeze(0).detach().cpu().numpy()\n",
    "            f.create_dataset(key, data=rep_np, compression=\"gzip\")\n",
    "            if index % 100 == 0:\n",
    "                print(f\"Processed {index} images, out of {len(train_df)}\")\n",
    "    #close the file\n",
    "    f.close()\n",
    "# Initialize a dictionary to store new feature columns\n",
    "else:\n",
    "    new_features = {}\n",
    "\n",
    "    for index,t in train_df.iterrows():\n",
    "        if huggingface:\n",
    "            if pooling:\n",
    "                print(\"Pooling is not implemented yet\")\n",
    "                break\n",
    "            else:\n",
    "                output = compute_output(model, device, transform, t, huggingface, patches)[:,0,:]\n",
    "        else:\n",
    "            output = compute_output(model, device, transform, t, huggingface, patches)\n",
    "        for i, value in enumerate(output.squeeze().tolist()):\n",
    "            column_name = f\"f{i+1}\"\n",
    "            if column_name not in new_features:\n",
    "                new_features[column_name] = []\n",
    "            new_features[column_name].append(value)\n",
    "        if index % 100 == 0:\n",
    "            print(f\"Processed {index} images, out of {len(train_df)}\")\n",
    "\n",
    "        \n",
    "    # Add the new features to the DataFrame in one operation\n",
    "    new_features_df = pd.DataFrame(new_features)\n",
    "    train_df = pd.concat([train_df.reset_index(drop=True), new_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e4c38b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>writer</th>\n",
       "      <th>isEng</th>\n",
       "      <th>same_text</th>\n",
       "      <th>file_name</th>\n",
       "      <th>male</th>\n",
       "      <th>train</th>\n",
       "      <th>index</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x2</th>\n",
       "      <th>...</th>\n",
       "      <th>f503</th>\n",
       "      <th>f504</th>\n",
       "      <th>f505</th>\n",
       "      <th>f506</th>\n",
       "      <th>f507</th>\n",
       "      <th>f508</th>\n",
       "      <th>f509</th>\n",
       "      <th>f510</th>\n",
       "      <th>f511</th>\n",
       "      <th>f512</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D:\\download\\PD project\\datasets\\ICDAR 2013 - G...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.171300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.539873</td>\n",
       "      <td>0.160685</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.000343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D:\\download\\PD project\\datasets\\ICDAR 2013 - G...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.277823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.518765</td>\n",
       "      <td>0.151740</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>0.000835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D:\\download\\PD project\\datasets\\ICDAR 2013 - G...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1479</td>\n",
       "      <td>493</td>\n",
       "      <td>1972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.716730</td>\n",
       "      <td>0.163569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D:\\download\\PD project\\datasets\\ICDAR 2013 - G...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>986</td>\n",
       "      <td>493</td>\n",
       "      <td>1479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.196669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.575615</td>\n",
       "      <td>0.183626</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.000950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D:\\download\\PD project\\datasets\\ICDAR 2013 - G...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1972</td>\n",
       "      <td>493</td>\n",
       "      <td>2465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.191164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.492923</td>\n",
       "      <td>0.086766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 524 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   writer  isEng  same_text  \\\n",
       "0       1      0          0   \n",
       "1       1      0          0   \n",
       "2       1      0          0   \n",
       "3       1      0          0   \n",
       "4       1      0          0   \n",
       "\n",
       "                                           file_name  male  train  index  \\\n",
       "0  D:\\download\\PD project\\datasets\\ICDAR 2013 - G...     0      1      0   \n",
       "1  D:\\download\\PD project\\datasets\\ICDAR 2013 - G...     0      1      1   \n",
       "2  D:\\download\\PD project\\datasets\\ICDAR 2013 - G...     0      1      2   \n",
       "3  D:\\download\\PD project\\datasets\\ICDAR 2013 - G...     0      1      3   \n",
       "4  D:\\download\\PD project\\datasets\\ICDAR 2013 - G...     0      1      4   \n",
       "\n",
       "      x    y    x2  ...  f503  f504      f505  f506      f507  f508      f509  \\\n",
       "0     0  493   493  ...   0.0   0.0  0.171300   0.0  0.005154   0.0  2.539873   \n",
       "1   493  493   986  ...   0.0   0.0  0.277823   0.0  0.009819   0.0  2.518765   \n",
       "2  1479  493  1972  ...   0.0   0.0  0.231340   0.0  0.030080   0.0  2.716730   \n",
       "3   986  493  1479  ...   0.0   0.0  0.196669   0.0  0.005942   0.0  2.575615   \n",
       "4  1972  493  2465  ...   0.0   0.0  0.191164   0.0  0.003046   0.0  2.492923   \n",
       "\n",
       "       f510      f511      f512  \n",
       "0  0.160685  0.000729  0.000343  \n",
       "1  0.151740  0.000764  0.000835  \n",
       "2  0.163569  0.000000  0.001213  \n",
       "3  0.183626  0.001086  0.000950  \n",
       "4  0.086766  0.000000  0.000961  \n",
       "\n",
       "[5 rows x 524 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11439c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 images, out of 5640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''from utils.dataframes import CustomImageDataset, CustomPatchDataset\n",
    "from torch.utils.data import Dataset,DataLoader, random_split\n",
    "source_path=\"D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\"\n",
    "train_df = pd.read_csv(f\"{source_path}\\\\outputs\\\\preprocessed_data\\\\{input_filename}\")\n",
    "model.eval()\n",
    "dataset = CustomPatchDataset(train_df[train_df['writer']<=N_max] ,\n",
    "                                        label_column='male', transform=transform, huggingface=huggingface)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "new_features = {}\n",
    "\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    patch = batch['image'].to(device)\n",
    "    output = model(pixel_values=patch)\n",
    "    cls_token_output = output.last_hidden_state[:, 0, :]  # Extract the CLS token\n",
    "    for i, value in enumerate(cls_token_output.squeeze().tolist()):\n",
    "        column_name = f\"f{i+1}\"\n",
    "        if column_name not in new_features:\n",
    "            new_features[column_name] = []\n",
    "        new_features[column_name].append(value)\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx * batch_size} images, out of {len(dataset)}\")\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "new_features_df = pd.DataFrame(new_features)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4647abe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to D:\\burtm\\Visual_studio_code\\PD_related_projects\\outputs\\preprocessed_data\\icdar_EXTRACTED_train_df_crnn_vgg16_bn_20250520_165111.csv\n"
     ]
    }
   ],
   "source": [
    "if not(save_h5):\n",
    "    from datetime import datetime\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = os.path.join(source_path, \"outputs\", \"preprocessed_data\")\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "    output_file = os.path.join(output_dir, f\"icdar_EXTRACTED_train_df_{selected_model}_{timestamp}.csv\")\n",
    "    train_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Dataframe saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b2060fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path=\"D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\"\n",
    "output_dir = os.path.join(source_path, \"outputs\", \"preprocessed_data\")\n",
    "if save_h5:\n",
    "    output_file = h5_file_name\n",
    "else:\n",
    "    output_file = os.path.join(output_dir, f\"icdar_EXTRACTED_train_df_{selected_model}_{timestamp}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f03398c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file path: D:\\burtm\\Visual_studio_code\\PD_related_projects\\outputs\\preprocessed_data\\file_metadata_log.json\n",
      "Output file path: D:\\burtm\\Visual_studio_code\\PD_related_projects\\outputs\\preprocessed_data\\icdar_EXTRACTED_train_df_crnn_vgg16_bn_20250520_165111.csv\n",
      "Updated log for icdar_EXTRACTED_train_df_crnn_vgg16_bn_20250520_165111.csv\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "LOG_FILE = output_dir+\"\\\\file_metadata_log.json\"\n",
    "print(f\"Log file path: {LOG_FILE}\")\n",
    "print(f\"Output file path: {output_file}\")\n",
    "file_IO.add_or_update_file(\n",
    "    output_file, LOG_FILE,\n",
    "    custom_metadata={\n",
    "        #\"seed\": seed,\n",
    "        \"source_file\": input_filename,\n",
    "        \"model\": selected_model,\n",
    "        \"pooling\": pooling,\n",
    "        \"custom transform\": custom_transform,\n",
    "        \"save_h5\": save_h5,\n",
    "        \"truncation\": truncation,\n",
    "        \"transform_mode\": transform_mode,\n",
    "        \"description\": '''\n",
    "        I am performing inference with a crnn model pretrained for handwriting recognition on the patches dataset the output is \n",
    "        a pooling of the concatenation of the output series\n",
    "        I am resizing the images to H/3 x W and then resizing to 32x128 with padding''' \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dd009b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for icdar_EXTRACTED_train_df_crnn_vgg16_bn_20250520_165111.csv:\n",
      "full_path: D:\\burtm\\Visual_studio_code\\PD_related_projects\\outputs\\preprocessed_data\\icdar_EXTRACTED_train_df_crnn_vgg16_bn_20250520_165111.csv\n",
      "size_bytes: 42963375\n",
      "created: 2025-05-20T16:51:11.102723\n",
      "modified: 2025-05-20T16:51:15.302572\n",
      "accessed: 2025-05-20T16:51:15.302572\n",
      "source_file: icdar_train_df_patches_20250515_164130.csv\n",
      "model: crnn_vgg16_bn\n",
      "pooling: False\n",
      "custom transform: True\n",
      "save_h5: False\n",
      "truncation: remove head\n",
      "transform_mode: padding\n",
      "description: \n",
      "        I am performing inference with a crnn model pretrained for handwriting recognition on the patches dataset the output is \n",
      "        a pooling of the concatenation of the output series\n",
      "        I am resizing the images to H/3 x W and then resizing to 32x128 with padding\n"
     ]
    }
   ],
   "source": [
    "file_IO.read_metadata(\n",
    "    output_file,\n",
    "    log_path=LOG_FILE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd41e5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: D:\\burtm\\Visual_studio_code\\PD_related_projects\\outputs\\preprocessed_data\\icdar_EXTRACTED_trocr_stage_1_10blocks_20250509_153553.csv\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "file_path=source_path+f\"\\\\outputs\\\\preprocessed_data\\\\icdar_EXTRACTED_trocr_stage_1_10blocks_{timestamp}.csv\"\n",
    "train_df.to_csv(file_path, index=False)\n",
    "print(f\"File saved: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14cff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b2dfb57",
   "metadata": {},
   "source": [
    "# easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "82e39392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_modules():\n",
    "    import importlib\n",
    "    import utils.data_loading as data_loading\n",
    "    import utils.visualization as visualization\n",
    "    import utils.dataframes as dataframes\n",
    "    import utils.utils_transforms as u_transforms\n",
    "    import utils.training_utils as training_utils\n",
    "    import utils.model_utils as model_utils\n",
    "    import utils.file_IO as file_IO\n",
    "    \n",
    "    importlib.reload(file_IO)\n",
    "    importlib.reload(data_loading)\n",
    "    importlib.reload(visualization)\n",
    "    importlib.reload(dataframes)\n",
    "    importlib.reload(u_transforms)\n",
    "    importlib.reload(model_utils)\n",
    "    importlib.reload(training_utils)\n",
    "\n",
    "    return data_loading, visualization, dataframes, u_transforms, training_utils, model_utils, file_IO\n",
    "data_loading, visualization, dataframes, u_transforms, training_utils, model_utils, file_IO = reload_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "807c5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output(model, device, transform, t, huggingface, patches):\n",
    "    image_file = t['file_name']\n",
    "    image = Image.open(image_file).convert(\"RGB\")\n",
    "    if patches:\n",
    "        x1 = t['x']\n",
    "        y1 = t['y']\n",
    "        x2 = t['x2']\n",
    "        y2 = t['y2']\n",
    "        patch = image.crop((x1, y1, x2, y2))\n",
    "    else:\n",
    "        patch = image.copy()\n",
    "    if huggingface:\n",
    "        # the transform is actually an huggingface processor in this case\n",
    "        inputs = transform(images=patch, return_tensors=\"pt\")\n",
    "        # Remove batch dimension from inputs\n",
    "        patch = inputs['pixel_values'].squeeze()\n",
    "    else:\n",
    "        patch = transform(patch)\n",
    "    patch = patch.to(device)\n",
    "    output = model(patch.unsqueeze(0))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dfdbc167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(transform, t, huggingface, patches):\n",
    "    image_file = t['file_name']\n",
    "    image = Image.open(image_file).convert(\"RGB\")\n",
    "    if patches:\n",
    "        x1 = t['x']\n",
    "        y1 = t['y']\n",
    "        x2 = t['x2']\n",
    "        y2 = t['y2']\n",
    "        patch = image.crop((x1, y1, x2, y2))\n",
    "    else:\n",
    "        patch = image.copy()\n",
    "\n",
    "    # Show original image/patch\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(patch)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    if huggingface:\n",
    "        inputs = transform(images=patch, return_tensors=\"pt\")\n",
    "        transformed_patch = inputs['pixel_values'].squeeze()\n",
    "        # Convert tensor to numpy for visualization\n",
    "    else:\n",
    "        transformed_patch = transform(patch)\n",
    "    \n",
    "    img_np = transformed_patch.permute(1, 2, 0).cpu().numpy()\n",
    "    # Normalize if needed\n",
    "    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img_np)\n",
    "    plt.title(\"Transformed\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a5cbb5",
   "metadata": {},
   "source": [
    "# save to h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(save_path+\"\\\\representations.h5\", \"r\") as f:\n",
    "    rep_42 = f[\"000042\"][:]  # Load representation for index 42\n",
    "print(rep_42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeneralPurposeML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
