{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1123c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "source_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ef851347",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_max=282\n",
    "patches=True\n",
    "input_filename='icdar_train_df_words_sentences_20250522_230307.csv'\n",
    "huggingface=False\n",
    "pooling=False # if true in transformer mdoels use pooling, if false only the cls token\n",
    "custom_transform=False\n",
    "transform_mode='resize'\n",
    "save_h5=False\n",
    "selected_model = 'vitstr_base'\n",
    "truncation = 'remove head'\n",
    "running = 'new-laptop'\n",
    "saved = 'old-laptop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9a2add41",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = u_transforms.get_transform(selected_model, use_patches=patches, custom=custom_transform, mode=transform_mode)\n",
    "model = model_utils.get_model(name=selected_model, mode='truncated', pretrained=True, truncation=truncation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a3b85963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is: \",device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "342c1500",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{source_path}\\\\outputs\\\\preprocessed_data\\\\{input_filename}\")\n",
    "train_df=file_IO.change_filename_from_to(train_df, fr=saved, to=running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f982df9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>writer</th>\n",
       "      <th>isEng</th>\n",
       "      <th>same_text</th>\n",
       "      <th>file_name</th>\n",
       "      <th>male</th>\n",
       "      <th>train</th>\n",
       "      <th>index</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>506</td>\n",
       "      <td>821</td>\n",
       "      <td>619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>628</td>\n",
       "      <td>821</td>\n",
       "      <td>739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1033</td>\n",
       "      <td>821</td>\n",
       "      <td>1139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>821</td>\n",
       "      <td>506</td>\n",
       "      <td>1642</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>821</td>\n",
       "      <td>641</td>\n",
       "      <td>1642</td>\n",
       "      <td>753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   writer  isEng  same_text  \\\n",
       "0       1      0          0   \n",
       "1       1      0          0   \n",
       "2       1      0          0   \n",
       "3       1      0          0   \n",
       "4       1      0          0   \n",
       "\n",
       "                                           file_name  male  train  index    x  \\\n",
       "0  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      1      0    0   \n",
       "1  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      1      1    0   \n",
       "2  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      1      2    0   \n",
       "3  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      1      3  821   \n",
       "4  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      1      4  821   \n",
       "\n",
       "      y    x2    y2  \n",
       "0   506   821   619  \n",
       "1   628   821   739  \n",
       "2  1033   821  1139  \n",
       "3   506  1642   629  \n",
       "4   641  1642   753  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "89e74c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualization.show_preprocessed_from_csv(train_df,transform,hugging=huggingface,index=1,patches=patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f0567a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import numpy as np\\nindex=0\\n# Select an image row from train_df\\nsample_row = train_df.iloc[index]\\nimage_file = sample_row[\\'file_name\\']\\nimage = Image.open(image_file).convert(\"RGB\")\\n\\n# If using patches, crop the patch, else use the whole image\\nif patches:\\n    x1, y1, x2, y2 = sample_row[\\'x\\'], sample_row[\\'y\\'], sample_row[\\'x2\\'], sample_row[\\'y2\\']\\n    patch = image.crop((x1, y1, x2, y2))\\nelse:\\n    patch = image.copy()\\n\\npatch=np.array(patch)\\npatch_list=[patch]\\nfrom doctr.models.preprocessor import PreProcessor\\ntransform=PreProcessor(\\n        (32, 128),\\n        batch_size=1,\\n        mean=(0.798, 0.785, 0.772),\\n        std=(0.264, 0.2749, 0.287),\\n        #preserve_aspect_ratio=True,\\n        #symmetric_pad=True,\\n    )\\npatch=transform(patch_list)\\npatch = patch[0].squeeze(0)\\nimg_np = patch.permute(1, 2, 0).cpu().numpy()\\nimg_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\\n\\nplt.imshow(img_np)\\nplt.title(\"Preprocessed Image\")\\nplt.axis(\\'off\\')\\nplt.show()'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dctocr test\n",
    "\n",
    "import numpy as np\n",
    "index=0\n",
    "# Select an image row from train_df\n",
    "sample_row = train_df.iloc[index]\n",
    "image_file = sample_row['file_name']\n",
    "image = Image.open(image_file).convert(\"RGB\")\n",
    "\n",
    "# If using patches, crop the patch, else use the whole image\n",
    "if patches:\n",
    "    x1, y1, x2, y2 = sample_row['x'], sample_row['y'], sample_row['x2'], sample_row['y2']\n",
    "    patch = image.crop((x1, y1, x2, y2))\n",
    "else:\n",
    "    patch = image.copy()\n",
    "\n",
    "patch=np.array(patch)\n",
    "patch_list=[patch]\n",
    "from doctr.models.preprocessor import PreProcessor\n",
    "transform=PreProcessor(\n",
    "        (32, 128),\n",
    "        batch_size=1,\n",
    "        mean=(0.798, 0.785, 0.772),\n",
    "        std=(0.264, 0.2749, 0.287),\n",
    "        #preserve_aspect_ratio=True,\n",
    "        #symmetric_pad=True,\n",
    "    )\n",
    "patch=transform(patch_list)\n",
    "patch = patch[0].squeeze(0)\n",
    "img_np = patch.permute(1, 2, 0).cpu().numpy()\n",
    "img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "\n",
    "plt.imshow(img_np)\n",
    "plt.title(\"Preprocessed Image\")\n",
    "plt.axis('off')\n",
    "plt.show()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0bd7fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "output=compute_output(model, device, transform, train_df.iloc[i], huggingface, patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "747a4420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Output shape: \", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "445a707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 images, out of 10152\n",
      "Processed 100 images, out of 10152\n",
      "Processed 200 images, out of 10152\n",
      "Processed 300 images, out of 10152\n",
      "Processed 400 images, out of 10152\n",
      "Processed 500 images, out of 10152\n",
      "Processed 600 images, out of 10152\n",
      "Processed 700 images, out of 10152\n",
      "Processed 800 images, out of 10152\n",
      "Processed 900 images, out of 10152\n",
      "Processed 1000 images, out of 10152\n",
      "Processed 1100 images, out of 10152\n",
      "Processed 1200 images, out of 10152\n",
      "Processed 1300 images, out of 10152\n",
      "Processed 1400 images, out of 10152\n",
      "Processed 1500 images, out of 10152\n",
      "Processed 1600 images, out of 10152\n",
      "Processed 1700 images, out of 10152\n",
      "Processed 1800 images, out of 10152\n",
      "Processed 1900 images, out of 10152\n",
      "Processed 2000 images, out of 10152\n",
      "Processed 2100 images, out of 10152\n",
      "Processed 2200 images, out of 10152\n",
      "Processed 2300 images, out of 10152\n",
      "Processed 2400 images, out of 10152\n",
      "Processed 2500 images, out of 10152\n",
      "Processed 2600 images, out of 10152\n",
      "Processed 2700 images, out of 10152\n",
      "Processed 2800 images, out of 10152\n",
      "Processed 2900 images, out of 10152\n",
      "Processed 3000 images, out of 10152\n",
      "Processed 3100 images, out of 10152\n",
      "Processed 3200 images, out of 10152\n",
      "Processed 3300 images, out of 10152\n",
      "Processed 3400 images, out of 10152\n",
      "Processed 3500 images, out of 10152\n",
      "Processed 3600 images, out of 10152\n",
      "Processed 3700 images, out of 10152\n",
      "Processed 3800 images, out of 10152\n",
      "Processed 3900 images, out of 10152\n",
      "Processed 4000 images, out of 10152\n",
      "Processed 4100 images, out of 10152\n",
      "Processed 4200 images, out of 10152\n",
      "Processed 4300 images, out of 10152\n",
      "Processed 4400 images, out of 10152\n",
      "Processed 4500 images, out of 10152\n",
      "Processed 4600 images, out of 10152\n",
      "Processed 4700 images, out of 10152\n",
      "Processed 4800 images, out of 10152\n",
      "Processed 4900 images, out of 10152\n",
      "Processed 5000 images, out of 10152\n",
      "Processed 5100 images, out of 10152\n",
      "Processed 5200 images, out of 10152\n",
      "Processed 5300 images, out of 10152\n",
      "Processed 5400 images, out of 10152\n",
      "Processed 5500 images, out of 10152\n",
      "Processed 5600 images, out of 10152\n",
      "Processed 5700 images, out of 10152\n",
      "Processed 5800 images, out of 10152\n",
      "Processed 5900 images, out of 10152\n",
      "Processed 6000 images, out of 10152\n",
      "Processed 6100 images, out of 10152\n",
      "Processed 6200 images, out of 10152\n",
      "Processed 6300 images, out of 10152\n",
      "Processed 6400 images, out of 10152\n",
      "Processed 6500 images, out of 10152\n",
      "Processed 6600 images, out of 10152\n",
      "Processed 6700 images, out of 10152\n",
      "Processed 6800 images, out of 10152\n",
      "Processed 6900 images, out of 10152\n",
      "Processed 7000 images, out of 10152\n",
      "Processed 7100 images, out of 10152\n",
      "Processed 7200 images, out of 10152\n",
      "Processed 7300 images, out of 10152\n",
      "Processed 7400 images, out of 10152\n",
      "Processed 7500 images, out of 10152\n",
      "Processed 7600 images, out of 10152\n",
      "Processed 7700 images, out of 10152\n",
      "Processed 7800 images, out of 10152\n",
      "Processed 7900 images, out of 10152\n",
      "Processed 8000 images, out of 10152\n",
      "Processed 8100 images, out of 10152\n",
      "Processed 8200 images, out of 10152\n",
      "Processed 8300 images, out of 10152\n",
      "Processed 8400 images, out of 10152\n",
      "Processed 8500 images, out of 10152\n",
      "Processed 8600 images, out of 10152\n",
      "Processed 8700 images, out of 10152\n",
      "Processed 8800 images, out of 10152\n",
      "Processed 8900 images, out of 10152\n",
      "Processed 9000 images, out of 10152\n",
      "Processed 9100 images, out of 10152\n",
      "Processed 9200 images, out of 10152\n",
      "Processed 9300 images, out of 10152\n",
      "Processed 9400 images, out of 10152\n",
      "Processed 9500 images, out of 10152\n",
      "Processed 9600 images, out of 10152\n",
      "Processed 9700 images, out of 10152\n",
      "Processed 9800 images, out of 10152\n",
      "Processed 9900 images, out of 10152\n",
      "Processed 10000 images, out of 10152\n",
      "Processed 10100 images, out of 10152\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "if save_h5:\n",
    "    import h5py\n",
    "    import numpy as np\n",
    "    import shutil\n",
    "    icdar_path=train_df['file_name'][0]\n",
    "    icdar_path = icdar_path[:icdar_path.lower().find(\"unzipped\")].rsplit(\"\\\\\", 1)[0] + \"\\\\\"\n",
    "    # Define the directory and file paths\n",
    "    h5_directory_name = \"extracted_representations_full\"\n",
    "    h5_save_path=icdar_path+h5_directory_name\n",
    "    # Open the file in append mode\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    h5_file_name = h5_save_path+f\"\\\\representations_{selected_model}_{truncation}_{timestamp}.h5\"\n",
    "    with h5py.File(h5_file_name, \"a\") as f:\n",
    "        model.eval()\n",
    "        for index,t in train_df.iterrows():\n",
    "            if huggingface:\n",
    "                output = compute_output(model, device, transform, t, huggingface, patches)\n",
    "            else:\n",
    "                output = compute_output(model, device, transform, t, huggingface, patches)\n",
    "            #print(output)\n",
    "            # Convert index to string key (e.g., \"0001\")\n",
    "            key = f\"{index:06d}\"\n",
    "            # Store with compression (optional)\n",
    "            rep_np = output.squeeze(0).detach().cpu().numpy()\n",
    "            f.create_dataset(key, data=rep_np, compression=\"gzip\")\n",
    "            if index % 100 == 0:\n",
    "                print(f\"Processed {index} images, out of {len(train_df)}\")\n",
    "    #close the file\n",
    "    f.close()\n",
    "# Initialize a dictionary to store new feature columns\n",
    "else:\n",
    "    new_features = {}\n",
    "\n",
    "    for index,t in train_df.iterrows():\n",
    "        if huggingface:\n",
    "            if pooling:\n",
    "                print(\"Pooling is not implemented yet\")\n",
    "                break\n",
    "            else:\n",
    "                output = compute_output(model, device, transform, t, huggingface, patches)[:,0,:]\n",
    "        else:\n",
    "            output = compute_output(model, device, transform, t, huggingface, patches)\n",
    "        for i, value in enumerate(output.squeeze().tolist()):\n",
    "            column_name = f\"f{i+1}\"\n",
    "            if column_name not in new_features:\n",
    "                new_features[column_name] = []\n",
    "            new_features[column_name].append(value)\n",
    "        if index % 100 == 0:\n",
    "            print(f\"Processed {index} images, out of {len(train_df)}\")\n",
    "\n",
    "        \n",
    "    # Add the new features to the DataFrame in one operation\n",
    "    new_features_df = pd.DataFrame(new_features)\n",
    "    train_df = pd.concat([train_df.reset_index(drop=True), new_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e4c38b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>writer</th>\n",
       "      <th>isEng</th>\n",
       "      <th>same_text</th>\n",
       "      <th>file_name</th>\n",
       "      <th>male</th>\n",
       "      <th>train</th>\n",
       "      <th>index</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x2</th>\n",
       "      <th>...</th>\n",
       "      <th>f2039</th>\n",
       "      <th>f2040</th>\n",
       "      <th>f2041</th>\n",
       "      <th>f2042</th>\n",
       "      <th>f2043</th>\n",
       "      <th>f2044</th>\n",
       "      <th>f2045</th>\n",
       "      <th>f2046</th>\n",
       "      <th>f2047</th>\n",
       "      <th>f2048</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1476</td>\n",
       "      <td>984</td>\n",
       "      <td>1722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527016</td>\n",
       "      <td>0.151143</td>\n",
       "      <td>0.052378</td>\n",
       "      <td>0.075106</td>\n",
       "      <td>0.139401</td>\n",
       "      <td>0.048882</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.026471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1722</td>\n",
       "      <td>984</td>\n",
       "      <td>1968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309595</td>\n",
       "      <td>0.589577</td>\n",
       "      <td>0.028709</td>\n",
       "      <td>0.582213</td>\n",
       "      <td>0.083180</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.087747</td>\n",
       "      <td>0.212132</td>\n",
       "      <td>0.081255</td>\n",
       "      <td>0.138128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>984</td>\n",
       "      <td>738</td>\n",
       "      <td>1230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038002</td>\n",
       "      <td>0.021267</td>\n",
       "      <td>0.020812</td>\n",
       "      <td>0.017402</td>\n",
       "      <td>0.129796</td>\n",
       "      <td>0.022709</td>\n",
       "      <td>0.002241</td>\n",
       "      <td>0.136042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>738</td>\n",
       "      <td>738</td>\n",
       "      <td>984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117669</td>\n",
       "      <td>0.031779</td>\n",
       "      <td>0.016809</td>\n",
       "      <td>0.109753</td>\n",
       "      <td>0.091854</td>\n",
       "      <td>0.043292</td>\n",
       "      <td>0.031907</td>\n",
       "      <td>0.400175</td>\n",
       "      <td>0.235237</td>\n",
       "      <td>0.021168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>492</td>\n",
       "      <td>738</td>\n",
       "      <td>738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210229</td>\n",
       "      <td>0.026293</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>0.008674</td>\n",
       "      <td>0.032707</td>\n",
       "      <td>0.018616</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.097478</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.106027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2061 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   writer  isEng  same_text  \\\n",
       "0       1      0          0   \n",
       "1       1      0          0   \n",
       "2       1      0          0   \n",
       "3       1      0          0   \n",
       "4       1      0          0   \n",
       "\n",
       "                                           file_name  male  train  index  \\\n",
       "0  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      1      0   \n",
       "1  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      1      1   \n",
       "2  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      1      2   \n",
       "3  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      1      3   \n",
       "4  C:\\Users\\andre\\PhD\\Datasets\\ICDAR 2013 - Gende...     0      1      4   \n",
       "\n",
       "      x    y    x2  ...     f2039     f2040     f2041     f2042     f2043  \\\n",
       "0  1476  984  1722  ...  0.527016  0.151143  0.052378  0.075106  0.139401   \n",
       "1  1722  984  1968  ...  0.309595  0.589577  0.028709  0.582213  0.083180   \n",
       "2   984  738  1230  ...  0.600109  0.000000  0.038002  0.021267  0.020812   \n",
       "3   738  738   984  ...  0.117669  0.031779  0.016809  0.109753  0.091854   \n",
       "4   492  738   738  ...  0.210229  0.026293  0.002361  0.008674  0.032707   \n",
       "\n",
       "      f2044     f2045     f2046     f2047     f2048  \n",
       "0  0.048882  0.001788  0.026471  0.000000  0.181974  \n",
       "1  0.102900  0.087747  0.212132  0.081255  0.138128  \n",
       "2  0.017402  0.129796  0.022709  0.002241  0.136042  \n",
       "3  0.043292  0.031907  0.400175  0.235237  0.021168  \n",
       "4  0.018616  0.001296  0.097478  0.000522  0.106027  \n",
       "\n",
       "[5 rows x 2061 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11439c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 images, out of 5640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''from utils.dataframes import CustomImageDataset, CustomPatchDataset\n",
    "from torch.utils.data import Dataset,DataLoader, random_split\n",
    "source_path=\"D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\"\n",
    "train_df = pd.read_csv(f\"{source_path}\\\\outputs\\\\preprocessed_data\\\\{input_filename}\")\n",
    "model.eval()\n",
    "dataset = CustomPatchDataset(train_df[train_df['writer']<=N_max] ,\n",
    "                                        label_column='male', transform=transform, huggingface=huggingface)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "new_features = {}\n",
    "\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    patch = batch['image'].to(device)\n",
    "    output = model(pixel_values=patch)\n",
    "    cls_token_output = output.last_hidden_state[:, 0, :]  # Extract the CLS token\n",
    "    for i, value in enumerate(cls_token_output.squeeze().tolist()):\n",
    "        column_name = f\"f{i+1}\"\n",
    "        if column_name not in new_features:\n",
    "            new_features[column_name] = []\n",
    "        new_features[column_name].append(value)\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx * batch_size} images, out of {len(dataset)}\")\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "new_features_df = pd.DataFrame(new_features)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b2060fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to c:\\Users\\andre\\VsCode\\PD related projects\\gender_detection\\outputs\\preprocessed_data\\icdar_EXTRACTED_train_df_vitstr_base_20250524_004825.csv\n"
     ]
    }
   ],
   "source": [
    "#source_path=\"D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\"\n",
    "output_dir = os.path.join(source_path, \"outputs\", \"preprocessed_data\")\n",
    "if save_h5:\n",
    "    output_file = h5_file_name\n",
    "else:\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = os.path.join(output_dir, f\"icdar_EXTRACTED_train_df_{selected_model}_{timestamp}.csv\")\n",
    "    train_df.to_csv(output_file, index=False)\n",
    "    print(f\"Dataframe saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f03398c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file path: c:\\Users\\andre\\VsCode\\PD related projects\\gender_detection\\outputs\\preprocessed_data\\file_metadata_log.json\n",
      "Output file path: c:\\Users\\andre\\VsCode\\PD related projects\\gender_detection\\outputs\\preprocessed_data\\icdar_EXTRACTED_train_df_vitstr_base_20250524_004825.csv\n",
      "Updated log for icdar_EXTRACTED_train_df_vitstr_base_20250524_004825.csv\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "LOG_FILE = output_dir+\"\\\\file_metadata_log.json\"\n",
    "print(f\"Log file path: {LOG_FILE}\")\n",
    "print(f\"Output file path: {output_file}\")\n",
    "file_IO.add_or_update_file(\n",
    "    output_file, LOG_FILE,\n",
    "    custom_metadata={\n",
    "        #\"seed\": seed,\n",
    "        \"source_file\": input_filename,\n",
    "        \"model\": selected_model,\n",
    "        \"pooling\": pooling,\n",
    "        \"custom transform\": custom_transform,\n",
    "        \"save_h5\": save_h5,\n",
    "        \"truncation\": truncation,\n",
    "        \"transform_mode\": transform_mode,\n",
    "        \"description\": '''\n",
    "        testing the vitstr model with the sentence level dataset''' \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd009b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for icdar_EXTRACTED_train_df_resnet50_20250523_175257.csv:\n",
      "full_path: c:\\Users\\andre\\VsCode\\PD related projects\\gender_detection\\outputs\\preprocessed_data\\icdar_EXTRACTED_train_df_resnet50_20250523_175257.csv\n",
      "size_bytes: 223055886\n",
      "created: 2025-05-23T17:52:57.692738\n",
      "modified: 2025-05-23T17:53:14.492980\n",
      "accessed: 2025-05-23T17:53:14.492980\n",
      "source_file: icdar_train_df_patches_20250521_120324.csv\n",
      "model: resnet50\n",
      "pooling: False\n",
      "custom transform: False\n",
      "save_h5: False\n",
      "truncation: remove head\n",
      "transform_mode: \n",
      "description: \n",
      "        testing resnet50 model on small patches datasets (1/100 the area of the original image)\n"
     ]
    }
   ],
   "source": [
    "file_IO.read_metadata(\n",
    "    output_file,\n",
    "    log_path=LOG_FILE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd41e5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: D:\\burtm\\Visual_studio_code\\PD_related_projects\\outputs\\preprocessed_data\\icdar_EXTRACTED_trocr_stage_1_10blocks_20250509_153553.csv\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "file_path=source_path+f\"\\\\outputs\\\\preprocessed_data\\\\icdar_EXTRACTED_trocr_stage_1_10blocks_{timestamp}.csv\"\n",
    "train_df.to_csv(file_path, index=False)\n",
    "print(f\"File saved: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14cff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b2dfb57",
   "metadata": {},
   "source": [
    "# easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82e39392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_modules():\n",
    "    import importlib\n",
    "    import utils.data_loading as data_loading\n",
    "    import utils.visualization as visualization\n",
    "    import utils.dataframes as dataframes\n",
    "    import utils.utils_transforms as u_transforms\n",
    "    import utils.training_utils as training_utils\n",
    "    import utils.model_utils as model_utils\n",
    "    import utils.file_IO as file_IO\n",
    "    \n",
    "    importlib.reload(file_IO)\n",
    "    importlib.reload(data_loading)\n",
    "    importlib.reload(visualization)\n",
    "    importlib.reload(dataframes)\n",
    "    importlib.reload(u_transforms)\n",
    "    importlib.reload(model_utils)\n",
    "    importlib.reload(training_utils)\n",
    "\n",
    "    return data_loading, visualization, dataframes, u_transforms, training_utils, model_utils, file_IO\n",
    "data_loading, visualization, dataframes, u_transforms, training_utils, model_utils, file_IO = reload_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "807c5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output(model, device, transform, t, huggingface, patches):\n",
    "    image_file = t['file_name']\n",
    "    image = Image.open(image_file).convert(\"RGB\")\n",
    "    if patches:\n",
    "        x1 = t['x']\n",
    "        y1 = t['y']\n",
    "        x2 = t['x2']\n",
    "        y2 = t['y2']\n",
    "        patch = image.crop((x1, y1, x2, y2))\n",
    "    else:\n",
    "        patch = image.copy()\n",
    "    if huggingface:\n",
    "        # the transform is actually an huggingface processor in this case\n",
    "        inputs = transform(images=patch, return_tensors=\"pt\")\n",
    "        # Remove batch dimension from inputs\n",
    "        patch = inputs['pixel_values'].squeeze()\n",
    "    else:\n",
    "        patch = transform(patch)\n",
    "    patch = patch.to(device)\n",
    "    output = model(patch.unsqueeze(0))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dfdbc167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(transform, t, huggingface, patches):\n",
    "    image_file = t['file_name']\n",
    "    image = Image.open(image_file).convert(\"RGB\")\n",
    "    if patches:\n",
    "        x1 = t['x']\n",
    "        y1 = t['y']\n",
    "        x2 = t['x2']\n",
    "        y2 = t['y2']\n",
    "        patch = image.crop((x1, y1, x2, y2))\n",
    "    else:\n",
    "        patch = image.copy()\n",
    "\n",
    "    # Show original image/patch\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(patch)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    if huggingface:\n",
    "        inputs = transform(images=patch, return_tensors=\"pt\")\n",
    "        transformed_patch = inputs['pixel_values'].squeeze()\n",
    "        # Convert tensor to numpy for visualization\n",
    "    else:\n",
    "        transformed_patch = transform(patch)\n",
    "    \n",
    "    img_np = transformed_patch.permute(1, 2, 0).cpu().numpy()\n",
    "    # Normalize if needed\n",
    "    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img_np)\n",
    "    plt.title(\"Transformed\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a5cbb5",
   "metadata": {},
   "source": [
    "# save to h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(save_path+\"\\\\representations.h5\", \"r\") as f:\n",
    "    rep_42 = f[\"000042\"][:]  # Load representation for index 42\n",
    "print(rep_42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeneralPurposeML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
