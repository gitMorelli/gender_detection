{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1123c1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\burtm\\Visual_studio_code\\conda_environments\\GeneralPurposeML\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(os.path.abspath(\"D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\"))\n",
    "\n",
    "from utils.model_utils import get_model, get_trainable_layers\n",
    "from utils.data_loading import get_dataloaders\n",
    "from utils.utils_transforms import get_transform  \n",
    "from utils.training_utils import fine_tune_last_n_layers, train_model, get_criterion, get_optimizer, get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "981875c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "import peft\n",
    "import bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a2add41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, PreTrainedModel, PretrainedConfig\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1',use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eee23cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.deit.modeling_deit.DeiTModel'> is overwritten by shared encoder config: DeiTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 384,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"deit\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.48.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 384,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 1024,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.48.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 64044\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd84d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5c5db7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeiTModel(\n",
      "  (embeddings): DeiTEmbeddings(\n",
      "    (patch_embeddings): DeiTPatchEmbeddings(\n",
      "      (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): DeiTEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x DeiTLayer(\n",
      "        (attention): DeiTSdpaAttention(\n",
      "          (attention): DeiTSdpaSelfAttention(\n",
      "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (output): DeiTSelfOutput(\n",
      "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): DeiTIntermediate(\n",
      "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): DeiTOutput(\n",
      "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "        (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "  (pooler): DeiTPooler(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f39a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_utils.TruncatedDeiT(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d800d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TruncatedDeiT(\n",
      "  (embeddings): DeiTEmbeddings(\n",
      "    (patch_embeddings): DeiTPatchEmbeddings(\n",
      "      (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): ModuleList(\n",
      "    (0-9): 10 x DeiTLayer(\n",
      "      (attention): DeiTSdpaAttention(\n",
      "        (attention): DeiTSdpaSelfAttention(\n",
      "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (output): DeiTSelfOutput(\n",
      "          (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): DeiTIntermediate(\n",
      "        (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): DeiTOutput(\n",
      "        (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "      (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a3f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dimension: torch.Size([1, 578, 384])\n",
      "tensor([[[-0.5542, -0.0727, -3.7304,  ...,  0.2753, -4.6295, -1.5863],\n",
      "         [ 2.3161, -3.4154, -5.5582,  ...,  2.1043, -5.9251, -0.5948],\n",
      "         [ 0.7380, -0.9215, -5.0391,  ...,  1.7634, -1.4387,  2.2261],\n",
      "         ...,\n",
      "         [ 1.5165, -1.5304,  0.0706,  ..., -1.8089, -3.3499, -0.4321],\n",
      "         [-4.1813,  1.7628, -5.0856,  ...,  0.9802, -1.0009,  2.6266],\n",
      "         [-1.3333,  1.1212, -4.7238,  ..., -1.5523,  0.1651,  0.9828]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a 384x384 input (batch size of 1, 3 color channels, 384x384 image)\n",
    "input_tensor = torch.randn(1, 3, 384, 384)\n",
    "\n",
    "# Perform inference with the model\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Print the output dimension\n",
    "print(\"Output dimension:\", output.shape)\n",
    "#print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f43944b",
   "metadata": {},
   "source": [
    "# standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec713cdf",
   "metadata": {},
   "source": [
    "how much time to fine tune the whole model? 60s per iteration -> 1h per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb1e68f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "N_max=282\n",
    "input_filename='icdar_train_df_cc_5patches_perName.csv'\n",
    "transform=processor\n",
    "huggingface=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3b85963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is: \",device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd41e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "source_path=\"D:\\\\burtm\\\\Visual_studio_code\\\\PD_related_projects\"\n",
    "file_path=source_path+f\"\\\\outputs\\\\preprocessed_data\\\\{input_filename}\"\n",
    "train_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "991cf750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "icdar_path=\"D:\\\\download\\\\PD project\\\\datasets\\\\ICDAR 2013 - Gender Identification Competition Dataset\\\\\"\n",
    "# Define the directory and file paths\n",
    "directory_name = \"extracted_representations_full\"\n",
    "save_path=icdar_path+directory_name\n",
    "log_file_name = \"log.txt\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Create a log file in the directory\n",
    "log_file_path = os.path.join(save_path, log_file_name)\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    log_file.write(\"Log data about the files in the folder:\\n\")\n",
    "    log_file.write(\"Each key in the hdf5 file is the full output of the trocr small stage 1 model truncated at the 10th transformer block \\n\")\n",
    "    log_file.write(\"The dataset corresponding to the data is the one reported below. The index of the file corresponds to the index in the df\\n\")\n",
    "\n",
    "# Copy the file to the directory\n",
    "source_file_path = file_path  # Assuming file_path is already defined\n",
    "destination_file_path = os.path.join(save_path, os.path.basename(file_path))\n",
    "shutil.copy(source_file_path, destination_file_path)\n",
    "\n",
    "# Add the copied file information to the log file\n",
    "with open(log_file_path, \"a\") as log_file:\n",
    "    log_file.write(f\"Copied file: {os.path.basename(file_path)}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b6eafba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 images, out of 5640\n",
      "Processed 100 images, out of 5640\n",
      "Processed 200 images, out of 5640\n",
      "Processed 300 images, out of 5640\n",
      "Processed 400 images, out of 5640\n",
      "Processed 500 images, out of 5640\n",
      "Processed 600 images, out of 5640\n",
      "Processed 700 images, out of 5640\n",
      "Processed 800 images, out of 5640\n",
      "Processed 900 images, out of 5640\n",
      "Processed 1000 images, out of 5640\n",
      "Processed 1100 images, out of 5640\n",
      "Processed 1200 images, out of 5640\n",
      "Processed 1300 images, out of 5640\n",
      "Processed 1400 images, out of 5640\n",
      "Processed 1500 images, out of 5640\n",
      "Processed 1600 images, out of 5640\n",
      "Processed 1700 images, out of 5640\n",
      "Processed 1800 images, out of 5640\n",
      "Processed 1900 images, out of 5640\n",
      "Processed 2000 images, out of 5640\n",
      "Processed 2100 images, out of 5640\n",
      "Processed 2200 images, out of 5640\n",
      "Processed 2300 images, out of 5640\n",
      "Processed 2400 images, out of 5640\n",
      "Processed 2500 images, out of 5640\n",
      "Processed 2600 images, out of 5640\n",
      "Processed 2700 images, out of 5640\n",
      "Processed 2800 images, out of 5640\n",
      "Processed 2900 images, out of 5640\n",
      "Processed 3000 images, out of 5640\n",
      "Processed 3100 images, out of 5640\n",
      "Processed 3200 images, out of 5640\n",
      "Processed 3300 images, out of 5640\n",
      "Processed 3400 images, out of 5640\n",
      "Processed 3500 images, out of 5640\n",
      "Processed 3600 images, out of 5640\n",
      "Processed 3700 images, out of 5640\n",
      "Processed 3800 images, out of 5640\n",
      "Processed 3900 images, out of 5640\n",
      "Processed 4000 images, out of 5640\n",
      "Processed 4100 images, out of 5640\n",
      "Processed 4200 images, out of 5640\n",
      "Processed 4300 images, out of 5640\n",
      "Processed 4400 images, out of 5640\n",
      "Processed 4500 images, out of 5640\n",
      "Processed 4600 images, out of 5640\n",
      "Processed 4700 images, out of 5640\n",
      "Processed 4800 images, out of 5640\n",
      "Processed 4900 images, out of 5640\n",
      "Processed 5000 images, out of 5640\n",
      "Processed 5100 images, out of 5640\n",
      "Processed 5200 images, out of 5640\n",
      "Processed 5300 images, out of 5640\n",
      "Processed 5400 images, out of 5640\n",
      "Processed 5500 images, out of 5640\n",
      "Processed 5600 images, out of 5640\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Open the file in append mode\n",
    "with h5py.File(save_path+\"\\\\representations.h5\", \"a\") as f:\n",
    "    model.eval()\n",
    "    for index,t in train_df.iterrows():\n",
    "        image_file = t['file_name']\n",
    "        x1 = t['x']\n",
    "        y1 = t['y']\n",
    "        x2 = t['x2']\n",
    "        y2 = t['y2']\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "        patch = image.crop((x1, y1, x2, y2))\n",
    "        if huggingface:\n",
    "            # the transform is actually an huggingface processor in this case\n",
    "            inputs = transform(images=patch, return_tensors=\"pt\")\n",
    "            # Remove batch dimension from inputs\n",
    "            patch = inputs['pixel_values'].squeeze()\n",
    "        elif transform:\n",
    "            patch = transform(patch)\n",
    "        patch = patch.to(device)\n",
    "        output = model(patch.unsqueeze(0))\n",
    "        #print(output)\n",
    "        # Convert index to string key (e.g., \"0001\")\n",
    "        key = f\"{index:06d}\"\n",
    "        # Store with compression (optional)\n",
    "        rep_np = output.squeeze(0).detach().cpu().numpy()\n",
    "        f.create_dataset(key, data=rep_np, compression=\"gzip\")\n",
    "        if index % 100 == 0:\n",
    "            print(f\"Processed {index} images, out of {len(train_df)}\")\n",
    "#close the file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a6714b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.7826767  -0.99325603 -4.8960156  ... -0.38691512 -2.6435657\n",
      "  -0.15363264]\n",
      " [ 3.6392565  -3.4912357  -6.4768376  ...  1.6145078  -5.1998186\n",
      "  -0.57736003]\n",
      " [-2.5747104   0.26719785 -4.02846    ... -1.2227541  -4.754792\n",
      "   1.9072549 ]\n",
      " ...\n",
      " [-2.774644    1.8033377  -5.8980827  ...  4.430866   -0.12699604\n",
      "   2.834492  ]\n",
      " [-0.9595611  -1.8759494  -2.069882   ... -4.8516946  -1.341036\n",
      "   3.092637  ]\n",
      " [-1.2976632   0.37103814 -4.6950593  ...  1.7340031  -1.1925094\n",
      "   1.6214855 ]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(save_path+\"\\\\representations.h5\", \"r\") as f:\n",
    "    rep_42 = f[\"000042\"][:]  # Load representation for index 42\n",
    "print(rep_42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2dfb57",
   "metadata": {},
   "source": [
    "# easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82e39392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_modules():\n",
    "    import importlib\n",
    "    import utils.data_loading as data_loading\n",
    "    import utils.visualization as visualization\n",
    "    import utils.dataframes as dataframes\n",
    "    import utils.utils_transforms as u_transforms\n",
    "    import utils.training_utils as training_utils\n",
    "    import utils.model_utils as model_utils\n",
    "    \n",
    "\n",
    "    importlib.reload(data_loading)\n",
    "    importlib.reload(visualization)\n",
    "    importlib.reload(dataframes)\n",
    "    importlib.reload(u_transforms)\n",
    "    importlib.reload(model_utils)\n",
    "    importlib.reload(training_utils)\n",
    "\n",
    "    return data_loading, visualization, dataframes, u_transforms, training_utils, model_utils\n",
    "data_loading, visualization, dataframes, u_transforms, training_utils, model_utils = reload_modules()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeneralPurposeML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
